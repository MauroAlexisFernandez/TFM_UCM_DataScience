{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba2e765-fa14-443a-aa9b-3162a51a0b2c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1><strong><u>TFM MÁSTER DATA SCIENCE, BIG DATA & BUSINESS ANALYTICS</u></strong></h1>\n",
    "    <h2><strong><u>Predicción de la suscripción a depósitos a plazo mediante Machine Learning y Deep Learning en campañas de marketing bancario </u></strong></h1>\n",
    "    <h3><em>MAURO ALEXIS FERNÁNDEZ</em></h2>\n",
    "    <h3><em>UNIVERSIDAD COMPLUTENSE DE MADRID</em></h2>\n",
    "    <h3><em>2024/2025</em></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df066c-bcd2-459a-b628-48e2574c88c8",
   "metadata": {},
   "source": [
    "\n",
    "**Link Repositorio Github del TFM** https://github.com/MauroAlexisFernandez/TFM_UCM_DataScience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e5bf8-a7e6-48b7-b06f-5c2562df4f33",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <strong>En virtud de los requerimientos detallados para la realización del TFM</strong><br>\n",
    "    Se presentan a continuación las líneas de código y comentarios relativos a la exploración y visualización de datos, procesamiento, modelado, ajuste de hiperparámetros y productivización del modelo predictivo. El código relativo a la parte ya productivizada del TFM, se incorpora en los otros archivos disponibles en el repositorio arriba referido.<br>\n",
    "    <strong>Fuente de datos disponible en: <a href=\"https://archive.ics.uci.edu/dataset/222/bank+marketing\" target=\"_blank\">Bank Marketing</a><br> </strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4728eb-0cb7-4f7a-a995-1b88a54829b8",
   "metadata": {},
   "source": [
    "**ÍNDICE**\n",
    "* [1. CARGA DE LAS LIBRERÍAS NECESARIAS Y EL DATASET](#section-one)\n",
    "* [2. ANÁLISIS EXPLORATORIO DE DATOS (EDA)](#section-two)\n",
    "    - [2.1 Distribución de la variable objetivo (desbalanceo)](#section-two-subsection-one)\n",
    "    - [2.2 Análisis de variables categóricas y numéricas](#section-two-subsection-two)\n",
    "    - [2.3 Correlaciones y relaciones significativas](#section-two-subsection-three)\n",
    "    - [2.4 Segmentaciones por atributos clave](#section-two-subsection-four)\n",
    "    - [2.5 Insights preliminares](#section-two-subsection-five)\n",
    "* [3. PREPROCESAMIENTO DE DATOS](#section-three)\n",
    "    - [3.1 Tratamiento de valores faltantes o inconsistentes](#section-three-subsection-one)\n",
    "    - [3.2 Encoding de variables categóricas](#section-three-subsection-two)\n",
    "    - [3.3 Transformaciones de variables numéricas](#section-three-subsection-three)\n",
    "* [4. INGENIERÍA DE CARACTERÍSTICAS](#section-four)\n",
    "     - [4.1 Agrupaciones o transformaciones lógicas](#section-four-subsection-one)\n",
    "     - [4.2 Generación de nuevas variables derivadas](#section-four-subsection-two)\n",
    "     - [4.3 Eliminación de variables redundantes](#section-four-subsection-three)\n",
    "     - [4.4 Encoding de variables categóricas (OneHotEncoder)](#section-four-subsection-four)   \n",
    "* [5. MODELADO PREDICTIVO](#section-five)\n",
    "     - [5.1 División del conjunto en entrenamiento y test](#section-five-subsection-one) \n",
    "     - [5.2 Elección de métricas: accuracy, precision, recall, F1, AUC](#section-five-subsection-two) \n",
    "     - [5.3 Definición del baseline](#section-five-subsection-three) \n",
    "     - [5.4 Modelo de Regresión logística](#section-five-subsection-four) \n",
    "     - [5.5 Modelo de Árboles de decisión](#section-five-subsection-five) \n",
    "     - [5.6 Modelo de Random Forest](#section-five-subsection-six) \n",
    "     - [5.7 Modelo de XGBoost](#section-five-subsection-seven) \n",
    "     - [5.8 Modelo de K-Nearest Neighbors](#section-five-subsection-eight) \n",
    "     - [5.9 Modelo de Naive Bayes](#section-five-subsection-nine)\n",
    "     - [5.10 Modelo de Deep Learning](#section-five-subsection-ten)\n",
    "     - [5.11 Comparación de modelos](#section-five-subsection-eleven)\n",
    "     - [5.12 Tuneo de hiperparámetros para el modelo seleccionado](#section-five-subsection-twelve)\n",
    "* [6. INTERPRETABILIDAD Y EXPLICABILIDAD DE MODELOS](#section-six)\n",
    "     - [6.1 Feature importance](#section-six-subsection-one)    \n",
    "     - [6.2 SHAP y LIME para interpretación local/global](#section-six-subsection-two) \n",
    "     - [6.3 Discusión sobre sesgos y robustez del modelo](#section-six-subsection-three)   \n",
    "* [7. EVALUACIÓN FINAL Y SELECCIÓN DE MODELO FINAL](#section-seven)\n",
    "     - [7.1 Comparación global de métricas](#section-seven-subsection-one)    \n",
    "     - [7.2 Selección del modelo con mejor balance interpretabilidad-rendimiento](#section-seven-subsection-two) \n",
    "* [8. PRODUCTIVIZACIÓN DEL MODELO](#section-eight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79ccf4-e7fe-4bb8-9464-819f5251932f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <strong>Descripción inicial del dataset y su propósito</strong><br>\n",
    "\n",
    "<em>\"Una institución bancaria portuguesa llevó a cabo campañas de marketing directo con el objetivo de evaluar si los clientes contratarían un depósito a plazo luego de ser contactados por los operadores comerciales.\n",
    "Dichas campañas se basaron en llamadas telefónicas, y en algunos casos fue necesario contactar al mismo cliente en más de una ocasión.\"</em></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb71d5a-e9f7-4b87-aa06-be29cd94f192",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\"> <strong>Contribución esperada</strong><br>\n",
    "\n",
    "<em>\"El presente trabajo tiene como objetivo construir y comparar modelos de clasificación que permitan predecir la suscripción a depósitos a plazo. Se espera identificar los atributos más influyentes en la decisión de los clientes y seleccionar un modelo robusto y explicable que pueda ser integrado en sistemas de apoyo a la decisión bancaria.\"</em></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa7271-80e4-4be8-87af-676376bb68b1",
   "metadata": {},
   "source": [
    "<a id=\"section-one\"></a>\n",
    "<h2><strong>1- CARGA DE LAS LIBRERÍAS NECESARIAS Y EL DATASET</strong> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0f3d2-1240-4701-8863-602957fa889a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "Se inicia el desarrollo del código mediante la siguiente celda donde se importarán la mayoría de las librerías necesarias para la realización de la actividad. Asimismo, se destaca que pueden existir repeticiones de importaciones en celdas posteriores (aunque se ha buscado minimizar las mismas).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def5898-25b9-491c-ab44-e73c6832ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "#Librerías básicas pandas y numpy de Python usadas para análisis de datos\n",
    "# ===============================\n",
    "import pandas as pd # Manipulación y análisis de datos tabulares\n",
    "import numpy as np # Operaciones numéricas y matrices\n",
    "\n",
    "# ===============================\n",
    "#Librerías de visualización\n",
    "# ===============================\n",
    "import matplotlib.pyplot as plt # Visualizaciones básicas (gráficos de línea, barras, histograma, etc.)\n",
    "import seaborn as sns # Visualizaciones estadísticas avanzadas, heatmaps, boxplots, etc.\n",
    "import plotly.express as px # Gráficos interactivos de alto nivel\n",
    "import plotly.graph_objects as go # Gráficos interactivos personalizados\n",
    "\n",
    "# ===============================\n",
    "#Librería para EDA (análisis exploratorio automatizado)\n",
    "# ===============================\n",
    "from ydata_profiling import ProfileReport # Generación automática de reportes de EDA\n",
    "\n",
    "# ===============================\n",
    "#Librería para transformar en numéricas aquellas variables categóricas \n",
    "#(se destaca que la ejecución de muchos modelos de ML no aceptan la introducción de variables categóricas)\n",
    "# ===============================\n",
    "from sklearn.preprocessing import LabelEncoder  # Codificación ordinal simple (útil para variables con orden implícito)\n",
    "from sklearn.preprocessing import OneHotEncoder # Codificación one-hot (útil para modelos que no aceptan etiquetas)\n",
    "\n",
    "# ===============================\n",
    "# Librerías para partición del dataset, validación cruzada, ajuste de hiperparámetros y validación de modelo\n",
    "# ===============================\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,  # División en conjuntos de entrenamiento y prueba\n",
    "    cross_val_score,   # Evaluación cruzada de modelos\n",
    "    GridSearchCV,      # Búsqueda de hiperparámetros óptimos\n",
    "    StratifiedKFold    # Validación cruzada estratificada\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier  # Clasificador de referencia (baseline)\n",
    "from sklearn.feature_selection import RFECV # Eliminación recursiva de características con validación cruzada\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # Clases base para crear transformadores personalizados (útiles en pipelines)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#Modelos clásicos de clasificación\n",
    "# ===============================\n",
    "from sklearn.linear_model import LogisticRegression # Regresión logística\n",
    "from sklearn.tree import DecisionTreeClassifier # Árboles de decisión\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier # Ensambles: Random Forest y AdaBoost\n",
    "from sklearn.svm import LinearSVC, SVC # Máquinas de vectores de soporte (lineales y no lineales)\n",
    "from xgboost import XGBClassifier # Clasificador basado en gradient boosting (XGBoost)\n",
    "from sklearn.naive_bayes import GaussianNB # Clasificador Naive Bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier # Clasificador K-Nearest Neighbors\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#Librerías de Deep Learning\n",
    "# ===============================\n",
    "import keras # Framework de alto nivel para redes neuronales (sobre TensorFlow)\n",
    "from keras.models import Sequential # Modelo secuencial de Keras\n",
    "from keras.layers import Dense, Dropout # Capa densa totalmente conectada y de dropout\n",
    "from tensorflow.keras import regularizers # Regularizadores L1, L2, etc.\n",
    "from keras import layers # Acceso directo a capas como Dense, Dropout, etc.\n",
    "from keras.metrics import Precision, Recall, AUC # Métricas específicas de Keras para evaluar modelos\n",
    "from keras.callbacks import EarlyStopping # Detiene el entrenamiento si no hay mejora en la métrica monitorizada\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Escalado de variables\n",
    "# ===============================\n",
    "from sklearn.preprocessing import MinMaxScaler # Escalado entre 0 y 1 (normalización)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Balanceo de clases con sobremuestreo\n",
    "# ===============================\n",
    "from imblearn.over_sampling import SMOTE # Técnica SMOTE para generar datos sintéticos de clases minoritarias\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Visualización de relaciones entre variables\n",
    "# ===============================\n",
    "from pandas.plotting import scatter_matrix # Matriz de diagramas de dispersión entre variables numéricas\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Selección de características\n",
    "# ===============================\n",
    "from sklearn.feature_selection import SelectKBest # Selección de las mejores K variables\n",
    "from sklearn.feature_selection import VarianceThreshold # Eliminación de variables con baja varianza\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Métricas de evaluación de modelos(curva ROC y área bajo la curva)\n",
    "# ===============================\n",
    "from sklearn.metrics import classification_report, accuracy_score, auc, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Estadísticas adicionales\n",
    "# ===============================\n",
    "from scipy.stats import stats # Pruebas estadísticas diversas (t-test, normalidad, etc.)\n",
    "from scipy.stats import chi2_contingency # Test de chi-cuadrado (asociación entre variables categóricas)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Interpretación de modelos (Explainability)\n",
    "# ===============================\n",
    "import lime  # Framework para interpretabilidad de modelos caja negra\n",
    "import shap  # Interpretación global y local de modelos (SHAP values)\n",
    "from lime import lime_tabular # Visualización local basada en LIME para datos tabulares\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Pipelines y serialización\n",
    "# ===============================\n",
    "from sklearn.pipeline import Pipeline # Creación de pipelines de preprocesamiento y modelado\n",
    "from sklearn.compose import ColumnTransformer # Aplicación de transformaciones por columnas\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Pipeline compatible con técnicas de sobremuestreo\n",
    "import joblib # Serialización de modelos y objetos\n",
    "import pickle # Serialización alternativa con Python puro\n",
    "\n",
    "# ===============================\n",
    "# Configuración de entorno\n",
    "# ===============================\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") # Ignorar advertencias para evitar ruido en la salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6bc566-1ec2-4cf8-841e-5c8f13843903",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "Se procede ahora a la lectura del archivo CSV con los datos originales y, a partir de los mismos, se genera un dataframe de pandas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d2c54-cd46-4c41-96ef-3ec3447d5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efectúo la lectura del csv y guardo todo en un dataframe de Pandas con nombre df\n",
    "df = pd.read_csv('bank-additional-full.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eaec73-2ce5-48f8-89d3-476afdd96ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso las primeras filas del dataframe generado\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3d549-b10c-44eb-8613-2c7475b10add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengo el tamaño y cantidad de columnas de dataframe recién generado\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25219ec6-de14-4255-8e16-98547c5cdb0f",
   "metadata": {},
   "source": [
    "A partir de los resultados obtenidos en la celda anterior, se observa que el dataframe contiene **41,188 registros** y **21 columnas** (incluyendo la variable objetivo).\n",
    "\n",
    "Este tamaño es representativo para el análisis que se llevará a cabo, permitiendo abordar un amplio rango de características para el modelado y evaluación de la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a825ddc-041b-4a66-be1f-18fb4118f184",
   "metadata": {},
   "source": [
    "<a id=\"section-two\"></a>\n",
    "<h2><strong>2 - ANÁLISIS EXPLORATORIO DE DATOS (EDA)</strong></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9acae2-eb0e-4e50-a0e6-0ead64a60fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso si la generación del dataframe df fue correcta y muestro todas las columnas presentes.\n",
    "#Selecciono la opción 'display.max_columns' para que pueda ver todas las columnas, sin truncar las intermedias con puntos suspensivos.\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547317a3-0f0f-4c2d-9b23-c34eac5c661c",
   "metadata": {},
   "source": [
    "## Identificación y descripción de las variables\n",
    "\n",
    "A continuación se incluye un listado con la descripción de las variables presentes en el dataset:\n",
    "\n",
    "### A. Datos sobre el cliente del banco\n",
    "\n",
    "| Variable   | Descripción                                                             | Tipo       | Nota |\n",
    "|------------|--------------------------------------------------------------------------|------------|------|\n",
    "| age        | Edad en años del cliente                                                        | Numérica   |      |\n",
    "| job        | Tipo de empleo del cliente                                                         | Categórica |      |\n",
    "| marital    | Estado civil del cliente                                                           | Categórica | *\"divorced\" puede incluir viudo* |\n",
    "| education  | Nivel educativo  del cliente                                                       | Categórica |      |\n",
    "| default    | ¿El cliente posee algún crédito en default?                                        | Categórica |      |\n",
    "| housing    | ¿El cliente posee algún crédito para vivienda?                                           | Categórica |      |\n",
    "| loan       | ¿El cliente posee algún crédito personal?                                                | Categórica |      |\n",
    "\n",
    "### B. Relativas al último contacto en la presente campaña\n",
    "\n",
    "| Variable     | Descripción                                                  | Tipo       |\n",
    "|--------------|---------------------------------------------------------------|------------|\n",
    "| contact      | Tipo de comunicación utilizada por el marketer                               | Categórica |\n",
    "| month        | Mes del último contacto con el cliente por el marketer                                      | Categórica |\n",
    "| day_of_week  | Día de la semana del último contacto por el marketer                         | Categórica |\n",
    "| duration     | Duración del último contacto (en segundos) entre cliente-marketer                   | Numérica   |\n",
    "\n",
    "### C. Otros atributos relacionados con la campaña\n",
    "\n",
    "| Variable   | Descripción                                                                 | Tipo       | Nota |\n",
    "|------------|------------------------------------------------------------------------------|------------|------|\n",
    "| campaign   | Cantidad de contactos realizados en esta campaña con el cliente                           | Numérica   |      |\n",
    "| pdays      | Días desde el último contacto en campañas anteriores con el cliente                       | Numérica   |*999 indica que no fue contactado previamente* |\n",
    "| previous   | Número de contactos previos antes de esta campaña con el cliente                          | Numérica   |      |\n",
    "| poutcome   | Resultado de campañas anteriores con este cliente                                            | Categórica |      |\n",
    "\n",
    "### D. Atributos socioeconómicos\n",
    "\n",
    "| Variable        | Descripción                                                             | Tipo       |\n",
    "|------------------|--------------------------------------------------------------------------|------------|\n",
    "| emp.var.rate     | Tasa de variación del empleo registrado – indicador trimestral                     | Numérica   |\n",
    "| cons.price.idx   | Índice de precios al consumidor – indicador mensual                     | Numérica   |\n",
    "| cons.conf.idx    | Índice de confianza del consumidor – indicador mensual                  | Numérica   |\n",
    "| euribor3m        | Tasa euribor a 3 meses – indicador diario                               | Numérica   |\n",
    "| nr.employed      | Número de personas empleadas – indicador trimestral                              | Numérica   |\n",
    "\n",
    "### E. Variable objetivo\n",
    "\n",
    "| Variable | Descripción                                          | Tipo     |\n",
    "|----------|-------------------------------------------------------|----------|\n",
    "| y        | ¿El cliente terminó por contratar un depósito a plazo fijo?        | Binaria  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd8275-290e-47bc-b956-ef4f6466a624",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "\n",
    "**Nota adicional 1:** \n",
    "\n",
    "Asimismo, se destaca el hecho de que el sitio, donde se aloja el dataset fuente, explicita que aquellos valores de **999** corresponden a **valores faltantes**.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d848efeb-4adc-426a-9e75-ff2093a3f4de",
   "metadata": {},
   "source": [
    "Ahora procedo a analizar e identificar la presencia de filas **duplicadas** en el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935f65b-6f41-43d0-baad-d6bb87b7ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero identifico las filas/registros duplicados\n",
    "duplicates = df[df.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647262f9-a9b2-4e43-8522-35f0615d7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muestro la cantidad de duplicados en el dataframe df\n",
    "print(f\"Cantidad de registros duplicados: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d59cf-2506-4864-bd03-3234dbadc990",
   "metadata": {},
   "source": [
    "Se detectaron **12 filas duplicadas** en el dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa031337-a40b-4c89-ad72-5dc3492ee316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se procede a eliminar aquellas filas previamente identificadas como duplicadas\n",
    "df.drop_duplicates(inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2bc31-0a9f-4e83-8d82-eeb16a150c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifico que no queden filas duplicadas \n",
    "duplicates = df[df.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73162d-e361-4852-8f24-0ef25587acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vuelvo a mostrar la cantidad de duplicados para revisar\n",
    "print(f\"Cantidad de registros duplicados: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823731ac-abcf-4186-a45d-ff5ae602c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observo el tamaño del dataframe luego de estas tareas de limpieza de duplicados\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc403d02-7837-49d2-8e7f-5e7af4fbc406",
   "metadata": {},
   "source": [
    "A continuación, procedo a analizar e identificar la presencia de **datos nulos** en el dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5878d1-b942-44bd-8e7b-20724c88e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afdb2e-bb49-4f92-9da8-081b03cd28f0",
   "metadata": {},
   "source": [
    "No se identificaron valores nulos **explícitos** en ninguna de las columnas del dataframe. Sin embargo, se procederá a investigar la posible existencia de valores faltantes **implícitos o codificados** de otra forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2a919-6e46-4829-82e0-bd2ad13b4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizo .info() sobre el dataframe para revisar los tipos de datos por variable en este momento.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877cb7d-bb66-4d8b-bf9e-fce189f06ab6",
   "metadata": {},
   "source": [
    "<a id=\"section-two-subsection-one\"></a>\n",
    "### **2.1 Distribución de la variable objetivo (desbalanceo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155e1e3-7d7a-466a-a92f-5d54e3cbfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Luego, busco obtener la distribución de valores entre las 2 categorías posibles para la variable objetivo (si se contratan o no créditos a plazo) .\n",
    "df.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc12524-22e8-40b5-ba3b-bef1a83873a7",
   "metadata": {},
   "source": [
    "Se observa un **marcado desbalance** en la distribución de la variable objetivo, con **36,537 registros correspondientes a la clase no y 4,639 registros a la clase yes**.\n",
    "Este desequilibrio deberá ser tenido en cuenta en las posteriores etapas de modelado, especialmente en las técnicas de balanceo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71954529-a045-4947-9cc4-0aac7359b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora lo presento de forma gráfica para evidenciarlo más claramente con una visualización\n",
    "# Primero cuento los valores\n",
    "y_counts = df['y'].value_counts()\n",
    "sizes = y_counts.values\n",
    "labels = y_counts.index\n",
    "\n",
    "colors = ['#66b3ff', '#ff9999']  # Defino los colores para cada valor de la variable\n",
    "\n",
    "def make_autopct(values):\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        count = int(round(pct * total / 100.0))\n",
    "        return f'{count} ({pct:.1f}%)'\n",
    "    return my_autopct\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "wedges, texts, autotexts = plt.pie(\n",
    "    sizes,\n",
    "    labels=None,  # No muestro las etiquetas fuera para no repetir y sobre-cargar la vista\n",
    "    autopct=make_autopct(sizes),\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    textprops={'color':'black'}  \n",
    ")\n",
    "\n",
    "plt.title('Distribución de la variable objetivo (y)')\n",
    "\n",
    "# Agrego leyenda que explique qué color corresponde a cada valor de y\n",
    "plt.legend(wedges, labels,\n",
    "           title=\"Valores de y\",\n",
    "           loc=\"center left\",\n",
    "           bbox_to_anchor=(1, 0, 0.5, 1))  # Lo coloco en una ubicación fuera del gráfico para no abigarrar demasiado\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89bd05-afa1-4379-b72e-9406bae04687",
   "metadata": {},
   "source": [
    "<a id=\"section-two-subsection-two\"></a>\n",
    "### **2.2 Análisis de variables categóricas y numéricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996647ab-57bb-4a1e-a424-6a9f03015fab",
   "metadata": {},
   "source": [
    "### Análisis de variables numéricas\n",
    "Se muestran a continuación las principales métricas descriptivas de las variables **numéricas**, incluyendo la media, los valores mínimo y máximo, así como los percentiles 25, 50 (mediana) y 75.  \n",
    "Esto permite obtener una primera aproximación a la distribución y posibles valores atípicos en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16f242-b4c4-407a-a09c-f62d9c9566c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecciono y separo las variables categóricas y numéricas del dataset xtrain\n",
    "cat_cols= df.select_dtypes(include=['object','category']).columns\n",
    "num_cols = df.select_dtypes(exclude=['object','category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53bf02-15de-4bcc-a6a5-f2887bb739fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con .describe() visualizo algunas métricas sobre las variables numéricas (media, valor mínimo,máximo, cuartiles, etc.)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f67ac4-598b-434a-90c0-8bb2bef55c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar histogramas y boxplots de variables numéricas\n",
    "def visualizar_numericas(df, num_cols):\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "\n",
    "        # Histograma\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[col], kde=True, bins=30, color='skyblue')\n",
    "        plt.title(f'Histograma de {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frecuencia')\n",
    "\n",
    "        # Boxplot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=df[col], color='salmon')\n",
    "        plt.title(f'Boxplot de {col}')\n",
    "        plt.xlabel(col)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Llamo a la función\n",
    "visualizar_numericas(df, num_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2da85-f4d6-4b7b-918d-5b2815331dff",
   "metadata": {},
   "source": [
    "### Análisis descriptivo de variables numéricas\n",
    "\n",
    "A partir del análisis preliminar, se observan los siguientes aspectos relevantes que serán fundamentales para la etapa de preprocesamiento:\n",
    "\n",
    "- **Edad (`age`)**: La distribución varía entre 17 y 98 años, con una media cercana a los 40 años y una desviación estándar de aproximadamente 10. Esto refleja una población mayoritariamente adulta y heterogénea. Podría considerarse agrupar por rangos etarios o detectar valores extremos.\n",
    "\n",
    "- **`campaign`**: Presenta una fuerte asimetría. El valor máximo es 56, mientras que el percentil 75 es apenas 3, lo que sugiere la presencia de *outliers* (clientes contactados muchas veces). Este comportamiento podría impactar negativamente en algunos algoritmos sensibles a valores extremos.\n",
    "\n",
    "- **`duration`**: La variable presenta una distribución fuertemente asimétrica hacia la derecha, con una gran concentración de observaciones en valores bajos (cercanos a cero) y una larga cola que se extiende hasta los 4918 segundos. El boxplot revela una abundante presencia de valores atípicos. \n",
    "\n",
    "- **`pdays`**: Se observa que los percentiles 25, 50 y 75 coinciden en 999, lo cual indica que este valor está siendo utilizado como código para \"no contactado previamente\". Por su naturaleza, debería ser recodificado o transformado en una variable categórica o binaria.\n",
    "\n",
    "- **`previous`**: Tanto la mediana como el tercer cuartil son cero, lo que indica que la mayoría de los clientes no tuvo contactos previos exitosos. Esta variable presenta una alta concentración de ceros, por lo que podría analizarse su distribución o convertirla en una variable binaria.\n",
    "\n",
    "- **Variables macroeconómicas (`emp.var.rate`, `cons.price.idx`, `euribor3m`, `nr.employed`)**: Muestran una baja dispersión relativa y valores bastante acotados, lo que refleja su estabilidad como indicadores de contexto económico. Si bien su variabilidad es limitada, podrían aportar valor al modelo al capturar el entorno macro durante cada campaña.\n",
    "\n",
    "En conjunto, este análisis permite identificar necesidades de transformación como recodificación de valores especiales, detección de outliers, y posibles decisiones de ingeniería de variables. Estos hallazgos guiarán el tratamiento adecuado de los datos en etapas posteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3b66a-0e7a-491e-8b46-1fee9c4247ce",
   "metadata": {},
   "source": [
    "### Análisis de variables categóricas\n",
    "\n",
    "A continuación, se procederá a analizar las variables categóricas del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301273d-d697-40d3-986b-69319c009cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo la distribución de todas las variables categóricas del dataframe\n",
    "for col in cat_cols:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    num_categories = df[col].nunique()\n",
    "    palette = sns.color_palette(\"husl\", num_categories)\n",
    "\n",
    "    sns.countplot(\n",
    "        x=col,\n",
    "        data=df,\n",
    "        order=df[col].value_counts().index,\n",
    "        palette=palette\n",
    "    )\n",
    "\n",
    "    plt.title(f'Conteo de valores para la variable categórica: {col}')\n",
    "    plt.xticks(rotation=45, ha='right')  # Con esto mejoro la legibilidad de aquellas etiquetas demasiado largas\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a82b21-40ed-48d2-a59b-eca6dab2d0a7",
   "metadata": {},
   "source": [
    "### Análisis descriptivo de variables categóricas\n",
    "\n",
    "En esta sección se analiza la distribución de las variables categóricas presentes en el dataset. Se utilizaron gráficos de barras (`countplot` de seaborn) para visualizar la frecuencia de cada categoría en las distintas variables, ordenando los valores de mayor a menor para facilitar su interpretación.\n",
    "\n",
    "A continuación se presentan las principales observaciones:\n",
    "\n",
    "### `poutcome` (Resultado de campañas anteriores)\n",
    "La mayoría de los clientes no ha participado en campañas previas (`nonexistent`). Las categorías `failure` y `success` tienen una representación significativamente menor. Podría considerarse recodificar esta variable en binaria (participó / no participó).\n",
    "\n",
    "### `job` (Empleos/ocupaciones)\n",
    "La mayoría de los clientes tienen ocupaciones como *admin.*, *blue-collar* y *technician*. También se observan categorías como *student*, *unemployed* y *unknown*, que podrían requerir un tratamiento especial.\n",
    "\n",
    "### `day_of_week` (Día del contacto)\n",
    "Distribución relativamente equilibrada entre los días de la semana, aunque martes y miércoles presentan una ligera mayor frecuencia. Puede mantenerse sin necesidad de transformación.\n",
    "\n",
    "### `month` (Mes del contacto)\n",
    "Se observa una fuerte concentración en los meses de mayo, junio, julio y agosto. Esto sugiere una estacionalidad en la ejecución de las campañas, lo cual podría ser relevante para el modelado.\n",
    "\n",
    "### `contact` (Tipo de contacto)\n",
    "La mayoría de los contactos se realizó vía celular. La categoría `telephone` tiene baja frecuencia. A pesar del desbalance, la variable puede aportar información útil.\n",
    "\n",
    "### `loan`, `housing`, `default`\n",
    "En los tres casos, predomina la categoría `no`. La variable `default` tiene una frecuencia muy baja en la categoría `yes`, lo que podría reducir su poder predictivo. Se recomienda evaluar su utilidad más adelante.\n",
    "\n",
    "### `education`\n",
    "Existen múltiples niveles educativos, con predominancia de `university.degree`, `high.school` y `basic.9y`. Algunas categorías como `illiterate` y `unknown` tienen muy poca representación. Sería conveniente agrupar niveles en categorías más amplias (ej.: básico, medio, superior) para reducir cardinalidad y mejorar interpretabilidad.\n",
    "\n",
    "### `marital`\n",
    "Distribución clara entre `married`, `single` y `divorced`, siendo `married` la más frecuente. Es una variable informativa que puede mantenerse sin modificaciones.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusión:**  \n",
    "Este análisis permitió identificar desbalances, patrones de estacionalidad y posibles oportunidades de transformación o agrupamiento en variables categóricas. Estas observaciones serán tenidas en cuenta en las etapas de preprocesamiento y modelado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04235ef-82cb-4757-a643-3d734764894d",
   "metadata": {},
   "source": [
    "### Generación de reporte/informe con el perfilado de datos\n",
    "En la siguiente celda, genero un informe con visualizaciones e información útil para profundizar esta etapa de EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d1c86-0350-43a1-a340-4696fa704afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta celda genera un informe con el perfilado de los datos. El archivo resultante puede ser consultado y explorado.\n",
    "#A partir de este informe, obtengo una serie de gráficos, recomendaciones y voy tomando nota de las caarcterísticas de los datos.\n",
    "prof = ProfileReport(df)\n",
    "prof.to_file(output_file='output_reporte.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52626f55-f53e-4e02-9df2-1ecebb6eaf4a",
   "metadata": {},
   "source": [
    "<a id=\"section-two-subsection-three\"></a>\n",
    "### **2.3 Correlaciones y relaciones significativas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5e74c-fa2e-40db-8a2f-66ddf8a622cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero un heatmap (mapa de calor) de las correlaciones entre las variables numéricas del DataFrame\n",
    "#Esto puede permitirme detectar variables altamente correlacionadas (multicolinealidad),\n",
    "#entender relaciones lineales entre features o tomar decisiones sobre selección de variables o ingeniería de features.\n",
    "%matplotlib inline\n",
    "df_correlacion = df.select_dtypes(include=['number'])\n",
    "correlation_mat = df_correlacion.corr()\n",
    "sns.heatmap(correlation_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d60da-6832-4086-98aa-7ca94fe3078c",
   "metadata": {},
   "source": [
    "### Análisis de correlación entre variables numéricas\n",
    "\n",
    "#### Variables incluidas:\n",
    "- `age`, `duration`, `campaign`, `pdays`, `previous`\n",
    "- `emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `euribor3m`, `nr.employed`\n",
    "\n",
    "---\n",
    "\n",
    "#### Observaciones del heatmap:\n",
    "\n",
    "-  **Alta correlación positiva** entre:\n",
    "  - `emp.var.rate`, `euribor3m`, `nr.employed` y `cons.price.idx`:\n",
    "    - Estas variables económicas están **fuertemente correlacionadas** entre sí (coef. > 0.8), lo que indica **riesgo de multicolinealidad** si se incluyen juntas en modelos lineales.\n",
    " \n",
    "-  **Correlaciones negativas destacadas**:\n",
    "  - `pdays` y `previous` tienen una fuerte correlación **negativa** (coef. < -0.5).\n",
    "  - `emp.var.rate` y `cons.conf.idx` también tienen una correlación negativa notable.\n",
    "\n",
    "-  **Variables independientes**:\n",
    "  - `age`, `duration` y `campaign` tienen **baja correlación** con las demás variables numéricas, lo cual es bueno si buscamos independencia entre features.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones que deberemos considerar:\n",
    "\n",
    "-  **Reducción de multicolinealidad**:\n",
    "   Puede ser necesario **dejar solo una o pocas** entre: `euribor3m`, `emp.var.rate`, `nr.employed` o `cons.price.idx` si se utiliza un modelo lineal o de regresión logística. Estas variables macro podrían ser muy útiles para explicar el contexto de la campaña, pero deben analizarse con cuidado por su interdependencia.\n",
    "\n",
    "-  **Transformaciones o selección**:\n",
    "   Para modelos basados en árboles (como XGBoost o Random Forest), **la multicolinealidad no es un problema grave**, aunque puede afectar la interpretabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc7f1e-ff5e-4a56-a73a-6c81e08e330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego y para observar las correlaciones entre las variables catagóricas, utilizo esta función para calcular la V de Cramér \n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k - 1, r - 1))\n",
    "\n",
    "# Creo matriz de Cramér's V entre variables categóricas\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "cramer_matrix = pd.DataFrame(index=cat_cols, columns=cat_cols)\n",
    "\n",
    "for col1 in cat_cols:\n",
    "    for col2 in cat_cols:\n",
    "        if col1 == col2:\n",
    "            cramer_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            try:\n",
    "                cramer_matrix.loc[col1, col2] = cramers_v(df[col1], df[col2])\n",
    "            except:\n",
    "                cramer_matrix.loc[col1, col2] = np.nan  # En caso de error (por ejemplo, columnas con un solo valor)\n",
    "\n",
    "cramer_matrix = cramer_matrix.astype(float)\n",
    "\n",
    "# Visualizo la matriz como heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramer_matrix, annot=True, cmap='YlGnBu', fmt='.2f', linewidths=0.5)\n",
    "plt.title(\"Matriz de asociación entre variables categóricas (Cramér's V)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28838279-9afc-406f-aacd-262da505e3ca",
   "metadata": {},
   "source": [
    "### Asociación entre variables categóricas (V de Cramer)\n",
    "\n",
    "Esta matriz muestra los niveles de asociación entre pares de variables categóricas mediante la **V de Cramer**, que toma valores entre 0 (sin relación) y 1 (asociación perfecta).\n",
    "\n",
    "---\n",
    "\n",
    "#### Observaciones clave:\n",
    "\n",
    "##### Alta asociación entre variables (V > 0.5):\n",
    "- `housing` y `loan`: **0.71**\n",
    "- `contact` y `month`: **0.61**\n",
    "\n",
    "Estas relaciones indican que los valores de una de las variables podrían anticiparse parcialmente con la otra. Puede evaluarse si alguna es redundante o si conviene tratarlas juntas.\n",
    "\n",
    "---\n",
    "\n",
    "##### Asociación moderada con la variable objetivo (`y`):\n",
    "- `poutcome` → `y`: **0.32** ➤ La más asociada con el target.\n",
    "- `month` → `y`: **0.27**\n",
    "- `poutcome` → `month`: **0.24**\n",
    "- `contact` → `y`: **0.14**\n",
    "- `job` → `y`: **0.15`\n",
    "\n",
    "Estas variables categóricas pueden ser relevantes para el modelo. Especial atención merece `poutcome`, que parece tener la mayor capacidad explicativa.\n",
    "\n",
    "---\n",
    "\n",
    "##### Asociación baja o nula (V < 0.1):\n",
    "- `day_of_week`, `default`, `loan`, `housing`, `marital` y `education` tienen poca asociación con `y` (V < 0.1).\n",
    "  - Aunque no se descartan, podrían tener menor prioridad en el modelado o en el análisis exploratorio.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones que debemos considerar:\n",
    "\n",
    "-  **Reducir redundancia**:\n",
    "   Revisar si conviene eliminar una entre `loan` y `housing`, o bien combinarlas.\n",
    "   Lo mismo aplica para `month` y `contact`.\n",
    "\n",
    "-  **Seleccionar variables con mayor poder predictivo**:\n",
    "   Priorizar `poutcome`, `month`, `job`, `contact`.\n",
    "\n",
    "   Si se usa un modelo como Random Forest o XGBoost, puede conservarse el conjunto completo, pero se recomienda revisar la **importancia de características** luego del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366a0c7-cfa8-48f0-b89b-e1488b093107",
   "metadata": {},
   "source": [
    "<a id=\"section-two-subsection-four\"></a>\n",
    "### **2.4 Segmentaciones por atributos clave**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddba2b6-af75-4ad2-9bde-2753557a1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En este punto asigno valores numéricos para cada una de los valores posibles que puede adoptar la variable objetivo.\n",
    "df.y.replace(('no','yes'),(0,1,),inplace=True)\n",
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17febb80-e3e9-4f58-a3c3-b4d6f938ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero un gráfico de torta de la variable objetivo\n",
    "\n",
    "target_labels = ['No', 'Yes']\n",
    "target_counts = [df.y[df.y == 0].count(), df.y[df.y == 1].count()]\n",
    "\n",
    "explode = (0, 0.25)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "cmap = plt.get_cmap('tab20')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 8)]\n",
    "\n",
    "# Creo el gráfico\n",
    "target_pie = plt.pie(\n",
    "    target_counts,\n",
    "    labels=target_labels,\n",
    "    explode=explode,\n",
    "    autopct='%1.2f%%',\n",
    "    textprops={'fontsize': 14},\n",
    "    shadow=True,\n",
    "    colors=colors\n",
    ")\n",
    "\n",
    "# Agrego el título\n",
    "plt.title('Distribución de la variable objetivo \"y\"', fontsize=18)\n",
    "\n",
    "# Agrego una leyenda explicativa\n",
    "plt.legend(\n",
    "    labels=[\"0 = No\", \"1 = Yes\"],\n",
    "    loc='lower left',\n",
    "    fontsize=12,\n",
    "    title=\"Codificación de y\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1081b-a089-4a6b-a9c9-093c4f6b11b0",
   "metadata": {},
   "source": [
    "### Segmentaciones por atributos clave de la variable job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b035a57-419a-45e5-a8d1-a8a335f29869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengo los valores posibles que adopta la variable job\n",
    "df.job.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c7b40-4220-43c2-9c4b-cb82cc287a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo la proporción de plazos suscriptos para cada valor posible de la columna trabajo\n",
    "job_proporcion = ((df.y[df.y == 1].groupby(df.job).count())/\n",
    "                  (df.y.groupby(df.job).count()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9cf9e-53b2-4db0-80c3-ef8ab3c59a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción de plazos suscriptos para cada tipo de trabajo\n",
    "job_proporcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5736c3b-87e7-4c2f-a376-294c6067e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero gráfico de la proporción de plazos suscritos (y=1) para cada tipo de trabajo\n",
    "job_rate = df.groupby('job')['y'].mean().sort_values()\n",
    "\n",
    "job_rate.plot(\n",
    "    kind='barh', figsize=(10,6), color='mediumseagreen', edgecolor='black'\n",
    ")\n",
    "plt.title('Proporción de éxito (y=1) por ocupación')\n",
    "plt.xlabel('Proporción de suscripción (\"sí\")')\n",
    "plt.ylabel('Ocupación')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995d62b-dbe0-4a43-a436-d997401fca21",
   "metadata": {},
   "source": [
    "#### Interpretación:\n",
    "\n",
    "- **Las ocupaciones con mayor proporción de éxito** (`y=1`) son:\n",
    "  - `\"student\"`: ≈ **31%**\n",
    "  - `\"retired\"`: ≈ **25%**\n",
    "  - `\"unemployed\"`: ≈ **14%**\n",
    "\n",
    "- **Las ocupaciones con menor proporción de éxito**:\n",
    "  - `\"blue-collar\"` y `\"services\"`: < **10%**\n",
    "  - `\"entrepreneur\"`, `\"housemaid\"`, `\"self-employed\"`, `\"technician\"`: también en torno al **10%**\n",
    "\n",
    "\n",
    "#### Conclusiones preliminares:\n",
    "\n",
    "- Hay **fuertes diferencias en la tasa de suscripción** según la ocupación.\n",
    "- **Estudiantes y jubilados son los grupos más propensos a suscribirse**, lo que puede deberse a mayor disponibilidad de tiempo, menor exposición a productos financieros previos o perfil de riesgo conservador.\n",
    "- Los trabajadores manuales y de servicios presentan **baja tasa de éxito**, posiblemente asociado a menores ingresos, educación financiera o desconfianza en instituciones bancarias.\n",
    "\n",
    "\n",
    "#### Recomendaciones/Ideas:\n",
    "\n",
    "- Esta variable es **altamente informativa** y debe incluirse en el modelo.\n",
    "- Puede ser útil agrupar ocupaciones en **niveles socioeconómicos** o en **segmentos de comportamiento financiero**.\n",
    "- Analizar si el patrón observado se mantiene en combinación con otras variables como edad o nivel educativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aedd6ad-5495-4fb5-8c44-33da3deb3262",
   "metadata": {},
   "source": [
    "### Segmentaciones por atributos clave de la variable age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18333fd-312e-4b65-82b4-bc1433dd0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero gráfico boxplot de la proporción de plazos suscritos (y=1) según edad\n",
    "plt.figure(figsize=(15,4))\n",
    "sns.boxplot(x='age', y='y', data=df, orient='h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9e258-23c3-4d98-bf24-4e9b6bdf1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizo la correlación entre edad y la variable objetivo\n",
    "df.age.corr(df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc4160-2aa9-4821-98a0-6eb8b429659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviso si hay correlación entre la variable objetivo frente a la edad y el hecho de que el cliente que sea jubilado\n",
    "df['age'][df.job == 'retired'].corr(df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918dd60-ecce-4ee7-9907-695e868d8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso el mismo criterio que la celda anterior pero con clientes que sean de ocupación estudiante.\n",
    "df['age'][df.job == 'student'].corr(df.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363689f-cfcf-4426-a896-ec1de2a3a2a6",
   "metadata": {},
   "source": [
    "#### Interpretación (según el boxplot):\n",
    "\n",
    "- Los **clientes que se suscribieron (`y = 1`) tienden a ser ligeramente mayores** en promedio que los que no lo hicieron (`y = 0`).\n",
    "- El rango intercuartílico (Q1–Q3) para `y = 1` está aproximadamente entre los **32 y 50 años**, con una mediana cercana a los **38–39 años**.\n",
    "- Para `y = 0`, la mediana está más cerca de los **35 años**, y el rango intercuartílico va aproximadamente de **30 a 45 años**.\n",
    "- Hay **outliers en ambos grupos** que superan los **70 años**, incluso hasta los **90+**, aunque representan una proporción baja del total.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones preliminares:\n",
    "\n",
    "- La variable `age` presenta una **distribución ligeramente desplazada hacia edades mayores** en los casos positivos (`y = 1`).\n",
    "- Esto sugiere que **las personas de mayor edad tienen más probabilidad de suscribirse**, aunque la diferencia no es drástica.\n",
    "- La dispersión es amplia, por lo que podría haber un efecto no lineal (por ejemplo, aumento de probabilidad hasta cierta edad y luego caída).\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones:\n",
    "\n",
    "- Incluir la variable `age` en el modelo, posiblemente como una **variable continua**.\n",
    "- Probar si **transformaciones no lineales** (como splines, categorización en tramos, o `age^2`) mejoran la capacidad predictiva.\n",
    "- También se puede combinar con otras variables como ocupación o estado civil para identificar perfiles específicos por grupo etario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d51d08-2564-4f58-ac7a-32725f3137db",
   "metadata": {},
   "source": [
    "### Segmentaciones por atributos clave de la variable education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c3d4e-e01e-4544-93e8-eba6f64323e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo la proporción de plazos suscriptos para cada valor posible de la columna educación\n",
    "edu_proporcion = ((df.y[df.y == 1].groupby(df.education).count())/\n",
    "                  (df.y.groupby(df.education).count()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374093c1-8837-4e5c-b816-3f9d2b03e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción de plazos suscriptos para cada nivel educativo\n",
    "edu_proporcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3ba47-3936-40f4-a9aa-8279813e5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero gráfico de la proporción de plazos suscritos (y=1) para cada nivel educativo\n",
    "education_rate = df.groupby('education')['y'].mean().sort_values()\n",
    "\n",
    "education_rate.plot(\n",
    "    kind='barh', figsize=(10,6), color='skyblue', edgecolor='black'\n",
    ")\n",
    "plt.title('Proporción de éxito (y=1) por nivel educativo')\n",
    "plt.xlabel('Proporción de suscripción (\"sí\")')\n",
    "plt.ylabel('Nivel educativo')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a96851-0b41-4a4c-8cce-9cff16c05daf",
   "metadata": {},
   "source": [
    "#### Interpretación:\n",
    "\n",
    "- Las **proporciones de suscripción (`y=1`)** varían significativamente según el nivel educativo:\n",
    "  - `\"illiterate\"`: ≈ **22%** *(mayor tasa de suscripción)*\n",
    "  - `\"unknown\"`: ≈ **14%**\n",
    "  - `\"university.degree\"`: ≈ **13.5%**\n",
    "  - `\"professional.course\"`, `\"high.school\"`: ≈ **11–12%**\n",
    "  - `\"basic\"` (4y, 6y, 9y): ≈ **8–10%**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones preliminares:\n",
    "\n",
    "- **Contrariamente a lo esperado**, los clientes analfabetos muestran la **mayor tasa de éxito**. Esto podría deberse a un tamaño muestral pequeño o a campañas especialmente dirigidas a ese grupo.\n",
    "- En general, a mayor nivel educativo, **mayor probabilidad de suscripción**, aunque no de forma lineal.\n",
    "- El grupo `\"unknown\"` tiene una proporción relativamente alta, lo cual sugiere que puede contener información útil y no debería eliminarse sin análisis adicional.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones/Ideas:\n",
    "\n",
    "- Esta variable tiene **valor predictivo moderado a alto** y debe incluirse en el modelo.\n",
    "- Puede ser útil agrupar los niveles en:\n",
    "  - Bajo (`basic`)\n",
    "  - Medio (`high.school`, `professional.course`)\n",
    "  - Alto (`university.degree`)\n",
    "  - Otros (`illiterate`, `unknown`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78555eff-b17e-4c53-8e40-a20e9b2da2c4",
   "metadata": {},
   "source": [
    "### Segmentaciones por atributos clave de la variable poutcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6b302-23cf-4bb9-994a-8c6fd3cd8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo la proporción de plazos suscriptos para cada valor posible de resultado de la campaña anterior\n",
    "poutcome_proporcion = ((df.y[df.y == 1].groupby(df.poutcome).count())/\n",
    "                      (df.y.groupby(df.poutcome).count()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5fc9a-accd-4036-8687-70b19860c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción de plazos suscriptos según resultados de campañas previas\n",
    "poutcome_proporcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c614b22-53c6-4daf-a75e-6bd9afb90776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero gráfico de la proporción de plazos suscritos (y=1) según resultados de campañas previas\n",
    "poutcome_rate = df.groupby('poutcome')['y'].mean().sort_values()\n",
    "\n",
    "poutcome_rate.plot(\n",
    "    kind='barh', figsize=(9,4), color='mediumvioletred', edgecolor='black'\n",
    ")\n",
    "plt.title('Proporción de éxito (y=1) según resultado de campaña anterior')\n",
    "plt.xlabel('Proporción de suscripción (\"sí\")')\n",
    "plt.ylabel('Resultado campaña anterior')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47697220-ba14-476f-8e98-990100b4dd38",
   "metadata": {},
   "source": [
    "#### Interpretación:\n",
    "\n",
    "- La **proporción de suscripción (`y=1`)** varía significativamente según el resultado de la campaña anterior:\n",
    "  - `\"success\"`: ≈ **65%**\n",
    "  - `\"failure\"`: ≈ **13%**\n",
    "  - `\"nonexistent\"`: ≈ **9%**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones preliminares:\n",
    "\n",
    "- Si la campaña anterior fue **exitosa**, hay **alta probabilidad de que el cliente vuelva a suscribirse** (~65%).\n",
    "- Si la campaña **no existió** o fue un **fracaso**, la probabilidad de éxito actual baja mucho.\n",
    "- Esta variable **es altamente informativa** y podría ser una de las más relevantes para predecir la variable objetivo.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones/Ideas:\n",
    "\n",
    "- Incluir esta variable en el modelo predictivo como un factor clave.\n",
    "- Puede ser útil generar una variable binaria adicional que indique si el cliente tuvo una campaña previa o no.\n",
    "- También puede ser útil explorar interacciones con el número de contactos o canal de comunicación para profundizar el análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84d5f9-a3a1-4d2d-b3e1-21cf7b5e716f",
   "metadata": {},
   "source": [
    "### Segmentaciones por atributos clave de la variable marital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ddcc5-6631-49df-9917-dee3bfa8148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo la proporción de plazos suscriptos para cada valor posible de estado civil\n",
    "marital_proporcion = ((df.y[df.y == 1].groupby(df.marital).count())/\n",
    "                      (df.y.groupby(df.marital).count()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71dab6-3b61-4bf8-8bc1-5af31990b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción de plazos suscriptos para cada valor posible de estado civil\n",
    "marital_proporcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7ccc2-c626-4606-aa6a-d0000bb74b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero gráfico de la proporción de plazos suscritos (y=1) para cada valor posible de estado civil\n",
    "marital_rate = df.groupby('marital')['y'].mean().sort_values()\n",
    "\n",
    "marital_rate.plot(\n",
    "    kind='barh', figsize=(10,6), color='salmon', edgecolor='black'\n",
    ")\n",
    "plt.title('Proporción de éxito (y=1) por estado civil')\n",
    "plt.xlabel('Proporción de suscripción (\"sí\")')\n",
    "plt.ylabel('Estado civil')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424c979-8199-4c15-b72e-2a3cf3229109",
   "metadata": {},
   "source": [
    "#### Interpretación:\n",
    "\n",
    "- La **proporción de suscripción (`y=1`)** varía levemente según el estado civil:\n",
    "  - `\"unknown\"`: ≈ **15%**\n",
    "  - `\"single\"`: ≈ **14%**\n",
    "  - `\"divorced\"`: ≈ **10.3%**\n",
    "  - `\"married\"`: ≈ **10.2%**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones preliminares:\n",
    "\n",
    "- Las personas **solteras o con estado civil desconocido** presentan **mayor tasa de suscripción** al depósito a plazo fijo.\n",
    "- Los **clientes casados o divorciados** muestran proporciones más bajas de éxito.\n",
    "- Aunque las diferencias no son enormes, esta variable podría aportar **valor moderado** al modelo.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones/Ideas:\n",
    "\n",
    "- Considerar esta variable como parte del modelo, especialmente si se combina con edad o ingresos.\n",
    "- Evaluar si la categoría `\"unknown\"` está sesgada hacia algún segmento particular (por ejemplo, omisiones sistemáticas en ciertas edades o profesiones).\n",
    "- Posible transformación: agrupar `\"married\"` y `\"divorced\"` como \"no solteros\", si mejora la capacidad predictiva del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf28cfc-9a9c-4f52-8157-759c6d8867ed",
   "metadata": {},
   "source": [
    "### Segmentaciones por atributos clave de la variable default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f02b30-3f8e-4202-a966-6abb9a857a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo la proporción de plazos suscriptos para cada valor posible de default\n",
    "default_proporcion = ((df.y[df.y == 1].groupby(df.default).count())/\n",
    "                      (df.y.groupby(df.default).count()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35d27e-9b06-47f3-a5c3-823efd96543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción de plazos suscriptos para cada valor posible de default\n",
    "default_proporcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27708ba3-75f0-48c2-8d1d-38360dac6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero gráfico de la proporción de plazos suscritos (y=1) para cada valor posible de default\n",
    "default_rate = df.groupby('default')['y'].mean().sort_values()\n",
    "\n",
    "default_rate.plot(\n",
    "    kind='barh', figsize=(8,4), color='tomato', edgecolor='black'\n",
    ")\n",
    "plt.title('Proporción de éxito (y=1) según historial de default')\n",
    "plt.xlabel('Proporción de suscripción (\"sí\")')\n",
    "plt.ylabel('¿Tiene créditos en default?')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a282f-ffaa-4045-84e2-0e68e44cd843",
   "metadata": {},
   "source": [
    "#### Interpretación:\n",
    "\n",
    "- **Proporción de suscripción (`y=1`)** por categoría:\n",
    "  - `\"no\"`: ≈ **12.9%**\n",
    "  - `\"unknown\"`: ≈ **5.1%**\n",
    "  - `\"yes\"`: ≈ **0%** *(sin suscripciones en esta categoría)*\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones preliminares:\n",
    "\n",
    "- **Clientes sin historial de default (`no`) tienen la mayor tasa de éxito**, lo cual es esperable.\n",
    "- **Ningún cliente con historial de default (`yes`) se ha suscrito**, lo que indica una **relación fuertemente negativa** con la variable objetivo.\n",
    "- `\"unknown\"` tiene una proporción intermedia pero significativamente más baja que `\"no\"`, lo que sugiere cierta incertidumbre o desconfianza.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones/Ideas:\n",
    "\n",
    "- Esta variable tiene **alto poder discriminante** y debe ser considerada clave en el modelo.\n",
    "- El valor `\"yes\"` puede ser un indicador crítico para **segmentar negativamente** la base de clientes.\n",
    "- Verificar el tamaño del grupo `\"yes\"`: si es muy pequeño, su impacto podría ser limitado pero igualmente útil para reglas de negocio.\n",
    "- Considerar imputar o categorizar `\"unknown\"` si se puede cruzar con otras variables de comportamiento financiero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71837ba9-bcc4-4c0e-bbf0-53b5987afe5a",
   "metadata": {},
   "source": [
    "### Segmentaciones por atributos clave de la variable housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac921871-3262-4579-9c0b-ebc4ba6dc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo la proporción de plazos suscriptos para cada valor posible de housing\n",
    "housing_proporcion = ((df.y[df.y == 1].groupby(df.housing).count())/\n",
    "                      (df.y.groupby(df.housing).count()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ed1e4-162e-4af7-bab2-81286dc5d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción de plazos suscriptos para cada valor posible de housing\n",
    "housing_proporcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61c7d4-c6e3-4680-9b99-097c311bf71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero gráfico de la proporción de plazos suscritos (y=1) para cada valor posible de housing\n",
    "housing_rate = df.groupby('housing')['y'].mean().sort_values()\n",
    "\n",
    "housing_rate.plot(\n",
    "    kind='barh', figsize=(8,4), color='steelblue', edgecolor='black'\n",
    ")\n",
    "plt.title('Proporción de éxito (y=1) según tenencia de préstamo hipotecario')\n",
    "plt.xlabel('Proporción de suscripción (\"sí\")')\n",
    "plt.ylabel('¿Tiene préstamo hipotecario?')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a73ddc-698d-409a-a011-49fe9b49d21a",
   "metadata": {},
   "source": [
    "#### Interpretación:\n",
    "\n",
    "- Las **proporciones de suscripción (`y=1`)** son similares entre las categorías:\n",
    "  - `\"yes\"`: ≈ **11.6%**\n",
    "  - `\"no\"`: ≈ **11.0%**\n",
    "  - `\"unknown\"`: ≈ **10.9%**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones preliminares:\n",
    "\n",
    "- La tenencia de un **préstamo hipotecario no parece ser un factor decisivo** para predecir si un cliente se suscribirá.\n",
    "- Las diferencias entre las categorías son **muy pequeñas**, por lo tanto esta variable tiene **bajo poder discriminante** por sí sola.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones/Ideas:\n",
    "\n",
    "- Esta variable podría **no tener un impacto fuerte de manera individual**, pero aún así incluirla en el modelo puede aportar valor en combinación con otras variables (por ejemplo: ingresos, edad, ocupación).\n",
    "- Es recomendable revisar si la categoría `\"unknown\"` representa un grupo particular de clientes, o simplemente datos faltantes que pueden ser imputados o tratados como una categoría aparte.\n",
    "- Considerar la creación de una variable binaria simplificada (`tiene_prestamo_hipotecario`) si se busca reducir dimensionalidad sin pérdida de información.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f4902-1d08-490b-af0f-6eb84ee5d2e7",
   "metadata": {},
   "source": [
    "### Segmentaciones por atributos clave de la variable loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ef1cc-df4f-4a82-98f1-e0762d758b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo la proporción de plazos suscriptos para cada valor posible de loan\n",
    "loan_proporcion = ((df.y[df.y == 1].groupby(df.loan).count())/\n",
    "                   (df.y.groupby(df.loan).count()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b7d09-8e0e-45ec-a121-e8b514119634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción de plazos suscriptos para cada valor posible de loan\n",
    "loan_proporcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66eee0d-c7f9-4ea9-8cee-dc3781d91bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero gráfico de la proporción de plazos suscritos (y=1) para cada valor posible de loan\n",
    "loan_rate = df.groupby('loan')['y'].mean().sort_values()\n",
    "\n",
    "loan_rate.plot(\n",
    "    kind='barh', figsize=(8,4), color='darkorange', edgecolor='black'\n",
    ")\n",
    "plt.title('Proporción de éxito (y=1) según tenencia de préstamo personal')\n",
    "plt.xlabel('Proporción de suscripción (\"sí\")')\n",
    "plt.ylabel('¿Tiene préstamo personal?')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd78ee-1313-4ba7-be55-f5b5972848a8",
   "metadata": {},
   "source": [
    "#### Interpretación:\n",
    "\n",
    "- La **proporción de suscripción (`y=1`)** es muy similar entre las tres categorías:\n",
    "  - `\"no\"`: ~10.9%\n",
    "  - `\"yes\"`: ~10.7%\n",
    "  - `\"unknown\"`: ~10.6%\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones preliminares:\n",
    "\n",
    "- La **tenencia de un préstamo personal no parece tener una influencia clara** sobre la probabilidad de suscripción. Las diferencias son mínimas.\n",
    "- Esta variable **no discrimina bien** entre quienes se suscriben y quienes no.\n",
    "- La categoría `\"unknown\"` tiene una proporción levemente más baja, aunque esto podría deberse a ruido o falta de datos.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomendaciones/Ideas:\n",
    "\n",
    "- Esta variable **podría tener baja importancia predictiva** en un modelo de clasificación.\n",
    "- Se recomienda explorar **interacciones con otras variables** (por ejemplo: edad, ingresos, estado civil) para detectar posibles patrones no lineales.\n",
    "- Evaluar si el valor `\"unknown\"` debe imputarse o conservarse como categoría separada, según el análisis de los datos faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433cb39-eccc-4350-b117-712e288af235",
   "metadata": {},
   "source": [
    "<a id=\"section-two-subsection-five\"></a>\n",
    "### **2.5 Insights preliminares**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3620701-66e0-4392-8ce7-0dc544e9ede2",
   "metadata": {},
   "source": [
    "#### A) Información general\n",
    "- El conjunto de datos contiene **21 variables** y **41.176 registros**.\n",
    "- No se detectan **valores nulos explícitos** ni **duplicados**, lo cual indica una buena calidad inicial del dataset.\n",
    "- Se identifican **10 variables numéricas**, **10 categóricas** y **1 booleana**.\n",
    "\n",
    "#### B) Variable objetivo (`y`)\n",
    "- La variable objetivo está **desbalanceada**, con una distribución marcada hacia la clase negativa (`no`), lo que implica la necesidad de utilizar **técnicas de balanceo** (como sobremuestreo, submuestreo o SMOTE) en la etapa de modelado.\n",
    "\n",
    "#### C) Valores atípicos y dominantes\n",
    "- La variable `pdays` muestra un valor dominante de **999**, que denota ausencia de contacto previo. Se deberá derivar una **variable binaria (`was_contacted`)** a partir de esta.\n",
    "- `campaign` y `previous` presentan distribuciones con valores extremos o altamente sesgadas, lo que sugiere la existencia de **outliers** que deben ser tratados o analizados.\n",
    "- `age` tiene un rango amplio (hasta 98 años), por lo que podría resultar útil **agrupar en tramos etarios** para facilitar su análisis e interpretación.\n",
    "\n",
    "#### D) Variables categóricas con valores `unknown`\n",
    "- Varias variables categóricas contienen el valor `'unknown'`, lo que representa **valores faltantes implícitos**. En particular:\n",
    "  - `default`, `education`, `job`, `contact`, `poutcome` y `housing`.\n",
    "- Evaluar si estos valores deben tratarse adecuadamente mediante **imputación, recategorización o exclusión**, dependiendo del análisis.\n",
    "\n",
    "#### E) Correlaciones relevantes\n",
    "- Se observan **altas correlaciones** entre variables macroeconómicas, como:\n",
    "  - `euribor3m` y `nr.employed`.\n",
    "  - `emp.var.rate` y `euribor3m`.\n",
    "- Esto sugiere **redundancia de información** y puede evaluarse el uso de **reducción de dimensionalidad** (PCA) o selección de atributos.\n",
    "\n",
    "#### F) Recomendaciones/Ideas generales para secciones posteriores\n",
    "- Debemos pensar si se deberán tratar explícitamente los valores `'unknown'` como datos faltantes.\n",
    "- Crear variables derivadas útiles (por ejemplo, binarizar `pdays` y agrupar `age`).\n",
    "- Abordar el desequilibrio de la variable objetivo antes del modelado.\n",
    "- Analizar y mitigar el impacto de valores extremos en algunas variables clave.\n",
    "- Evaluar la pertinencia de mantener variables altamente correlacionadas o aplicar técnicas de reducción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2134e4-8630-47d2-a1ea-ff8ab374e575",
   "metadata": {},
   "source": [
    "<a id=\"section-three\"></a>\n",
    "<h2><strong>3 - PREPROCESAMIENTO DE DATOS</strong></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad1b05-60a3-43ad-b114-fd4244014483",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "A partir de aquí, comienzo a realizar una serie de preprocesamientos sobre el dataframe df para adecuar los datos.  \n",
    "\n",
    "Algunas modificaciones involucran:  \n",
    "- **Imputación o modificación de valores faltantes o inconsistentes (tanto explícitos como implícitos)**,\n",
    "- **Encoding de variables categóricas**,\n",
    "- **Transformación de variables numéricas**\n",
    "  \n",
    "Posteriormente, se realizarán tareas de **feature engineering**, donde se buscará crear nuevas variables derivadas o transformar las existentes para mejorar el rendimiento del modelo.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f5896-4fee-4f8a-9aed-de1fa59cdd0f",
   "metadata": {},
   "source": [
    "<a id=\"section-three-subsection-one\"></a>\n",
    "### **3.1 Tratamiento de valores faltantes o inconsistentes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c97a51-7894-404a-8046-49cdca0c33f4",
   "metadata": {},
   "source": [
    "### Imputación de valores faltantes en job\n",
    "\n",
    "Ahora, para inferir los valores faltantes en \"job\" y \"education\", utilizo la tabulación cruzada entre ambos. La hipótesis a considerar es que el \"job\" se ve influenciado por la \"education\" de una persona. Por lo tanto, es factible inferir \"job\" basándose en el nivel educativo de la persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfee12f-e9fa-4f83-8127-6b2537670acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero una función para una tabla cruzada entre dos variables categóricas del DataFrame.\n",
    "# Para cada categoría de la segunda variable (f2), se calcula cuántas veces aparece cada categoría de la primera variable (f1).\n",
    "# Devuelve un DataFrame donde las filas son las categorías de f1, las columnas son las de f2, y los valores representan los conteos.\n",
    "def cross_tab(df,f1,f2):\n",
    "    jobs=list(df[f1].unique())\n",
    "    edu=list(df[f2].unique())\n",
    "    dataframes=[]\n",
    "    for e in edu:\n",
    "        dfe=df[df[f2]==e]\n",
    "        dfejob=dfe.groupby(f1).count()[f2]\n",
    "        dataframes.append(dfejob)\n",
    "    xx=pd.concat(dataframes,axis=1)\n",
    "    xx.columns=edu\n",
    "    xx=xx.fillna(0)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a4a4d-3aa2-4c60-9091-0a22c59cbb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, muestro la cantidad de personas por combinación de job y education.\n",
    "cross_tab(df,'job','education')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20128f40-31a3-48fb-9390-72e22edcabfb",
   "metadata": {},
   "source": [
    "También propongo que personas cuya edad sea mayor a 60 probablemente sea jubilado(retired) en caso de ser desconocida su situación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e4e7f-cc6c-4737-861e-77e1fd68f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de categorías de job entre personas mayores de 60 años.\n",
    "df['job'][df['age']>60].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8797c5-79e5-41cc-901d-dcac22c7523a",
   "metadata": {},
   "source": [
    "**Inferir la educación a partir de los empleos**: De la tabulación cruzada, se observa que las personas con puestos directivos suelen tener un título universitario. Por lo tanto, donde 'job' = management y 'education' = unknown, podemos sustituir 'education' por 'university.degree'. De forma similar, 'job' = 'services' --> 'education' = 'high.school' y 'job' = 'housemaid' --> 'education' = 'basic.4y'.\n",
    "\n",
    "**Inferir los empleos a partir de la educación**: Si 'education' = 'basic.4y', 'basic.6y' o 'basic.9y', el 'job' suele ser 'blue-collar'. Si 'education' = 'professional.course', el 'job' es 'technician'.\n",
    "\n",
    "**Inferir los empleos a partir de la edad**: Como se ve, si 'age' > 60, el 'job' es 'retireed', lo cual tiene sentido.\n",
    "\n",
    "Al imputar los valores de empleo y educación, tuve en cuenta que las correlaciones debían ser coherentes con la realidad. Si no lo eran, no reemplazaba los valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac6961-4385-487a-9dc3-34c0599f66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero una serie de transformaciones que imputen valores en 'job' cuando se den ciertas condiciones en otras variables\n",
    "df.loc[(df['age']>60) & (df['job']=='unknown'), 'job'] = 'retired'\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='management'), 'education'] = 'university.degree'\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='services'), 'education'] = 'high.school'\n",
    "df.loc[(df['education']=='unknown') & (df['job']=='housemaid'), 'education'] = 'basic.4y'\n",
    "df.loc[(df['job'] == 'unknown') & (df['education']=='basic.4y'), 'job'] = 'blue-collar'\n",
    "df.loc[(df['job'] == 'unknown') & (df['education']=='basic.6y'), 'job'] = 'blue-collar'\n",
    "df.loc[(df['job'] == 'unknown') & (df['education']=='basic.9y'), 'job'] = 'blue-collar'\n",
    "df.loc[(df['job']=='unknown') & (df['education']=='professional.course'), 'job'] = 'technician'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be0f33-7fe9-4060-be8c-694a8dc3f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vuelvo a mostrar la cantidad de personas por combinación de job y education.\n",
    "cross_tab(df,'job','education')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0079f3-c6af-40a6-8576-cd4383615667",
   "metadata": {},
   "source": [
    "De esta manera, se reduce el número de 'unknowns' y mejoro el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec191d-4a65-410b-b11b-d450012a59b1",
   "metadata": {},
   "source": [
    "### Imputación valores faltantes pdays\n",
    "\n",
    "Según la fuente de los datos (Repositorio de ML de U.C. Irvine), los valores faltantes, o NaN, se codifican como '999'. De la tabla anterior, se desprende claramente que solo 'pdays' presenta valores faltantes. Además y en virtud a lo arriba explicitado, la mayoría de los valores de 'pdays' están faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdde9c2-a69c-4f16-861d-7fa29ae53ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso esta función para crear un histograma con matplotlib para visualizar la distribución de una variable numérica.\n",
    "def drawhist(df, feature):\n",
    "    plt.hist(df[feature], bins=20, edgecolor='black')\n",
    "    plt.title(f'Distribución de {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b0dce-9bf8-42fe-84ea-51bf0966420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestro el histograma de la variable 'pdays', incluyendo todos los valores (incluido 999).\n",
    "drawhist(df,'pdays')\n",
    "plt.show()\n",
    "# Muestro el histograma de 'pdays' excluyendo los valores 999 (que indican que no hubo contacto previo).\n",
    "plt.hist(df.loc[df.pdays != 999, 'pdays'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb140ba6-5505-4e6e-9e7c-b5d3a50e7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una tabla cruzada entre 'pdays' y 'poutcome', mostrando la proporción (normalizada) de registros por combinación.\n",
    "# Se usa 'age' como columna de referencia solo para contar, sin afectar el resultado.\n",
    "pd.crosstab(df['pdays'],df['poutcome'], values=df['age'], aggfunc='count', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270c6e8-9cb9-4400-af26-c8e4f433691c",
   "metadata": {},
   "source": [
    "Como se puede observar en la tabla anterior, la mayoría de los valores de \"pdays\" faltan. La mayoría de estos valores faltantes se producen cuando el \"poutcome\" es un valor faltante. Esto significa que la mayoría de los valores de \"pdays\" faltan porque nunca se contactó al cliente. \n",
    "Para abordar esta variable, se elimina la variable numérica \"pdays\" y se la reemplaza por el valor 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2bd9a-8bf9-47cb-93f0-23fa24f3c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de los valores 999 de la variable 'pdays'\n",
    "df['pdays'] = df.pdays.map(lambda x: 0 if x == 999 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d790be4-0434-44e5-b294-fbe282d876c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestro nuevamente el histograma de la variable 'pdays', incluyendo todos los valores (incluido 999).\n",
    "drawhist(df,'pdays')\n",
    "plt.show()\n",
    "# Muestro nuevamente el histograma de 'pdays' excluyendo los valores 999 (que indican que no hubo contacto previo).\n",
    "plt.hist(df.loc[df.pdays != 999, 'pdays'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb581b-32ed-40cb-88d3-a518a725f45a",
   "metadata": {},
   "source": [
    "### Nota sobre Valores atípicos \n",
    "\n",
    "Los valores atípicos se definen como 1,5 x valor Q3 (percentil 75). De las visualizaciones y análisis en la etapa de EDA, se observa que solo 'age' y 'campaign' presentan valores atípicos como max('age') y max('campaign') > 1,5Q3('age') y >1,5Q3('campaign'), respectivamente.\n",
    "\n",
    "Pero también observamos que el valor de estos valores atípicos es bastante realista (max('age') = 98 y max('campaign') = 56). Por lo tanto, no es necesario eliminarlos, ya que el modelo de predicción debe representar el mundo real. Esto mejora la generalización del modelo y lo hace robusto para situaciones reales. Por lo tanto y en este caso, los valores atípicos no se eliminarán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed84a97-9f52-47b0-abff-d8ee91535700",
   "metadata": {},
   "source": [
    "<a id=\"section-three-subsection-two\"></a>\n",
    "### **3.2 Encoding de variables categóricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd4e81-9cb0-4be9-9ce5-6d7d6fafffed",
   "metadata": {},
   "source": [
    "### Encoding de las variables relativas a días de la semana y meses del año  \n",
    "\n",
    "Dado que tanto los días de la semana como los meses poseen un ordenamiento,considero codificarlos según su orden natural.\n",
    "Pero en primer lugar procedo a visualizar la distribución de valores para cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116d97e-ef58-4f48-83f8-ca6e9626f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso la distribución de valores en la variable month\n",
    "df['month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02bb6b1-c1f2-47f1-aa64-6411ca51fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso la distribución de valores en la variable day_of_week\n",
    "df['day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af454dd-59eb-4a13-9a22-43ed54fd46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En la variable  month y day_of_week, cambio sus valores categóricos (y en string) por valores numéricos.\n",
    "#Esta transformación con los números respectivos para los meses y días suele usarse para el paso de este tipo de variables categóricas a numéricas. \n",
    "\n",
    "df.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),\n",
    "                      (1,2,3,4,5,6,7,8,9,10,11,12),inplace=True)\n",
    "df.day_of_week.replace(('mon','tue','wed','thu','fri','sat','sun'),\n",
    "                      (1,2,3,4,5,6,7),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98d0e0-de1c-4adf-8f0a-dc1bcb94f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso que la transfomración de la variable month haya sido exitosa \n",
    "df['month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560bb0c-6ab0-4bfa-8bab-6808f6075677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso que la transfomración de la variable day_of_week haya sido exitosa \n",
    "df['day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9faa7-c5f8-4bf1-9d76-a7bc5f52463f",
   "metadata": {},
   "source": [
    "<a id=\"section-three-subsection-three\"></a>\n",
    "### **3.3 Transformaciones de variables numéricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dde89-db8e-4bcd-b71d-c0ea01c6f46c",
   "metadata": {},
   "source": [
    "En sección me propongo transformar aquella variable numérica que pueda beneficiarse de dicho procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0153c4-27e3-410d-93ce-ebf706558838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraigo las variables numéricas del dataset y obtengo información relevante para analizar\n",
    "numerical_variables = ['age','campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx','cons.conf.idx','euribor3m',\n",
    "                      'nr.employed']\n",
    "df[numerical_variables].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a266c82-38c4-410a-954e-23b34e46e95f",
   "metadata": {},
   "source": [
    "Observo que la variable 'age' posee una gran amplitud,la cual podría segmentarse en una nueva variable asociada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55304b54-a804-4ed0-a4cb-8e731685c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino los límites de edad para cada grupo y sus etiquetas\n",
    "bins = [0, 25, 40, 60, 140]\n",
    "labels = ['young', 'lower middle aged', 'middle aged', 'senior']\n",
    "\n",
    "# Genero la columna 'age_binned' con los grupos etiquetados sobre la columna 'age'\n",
    "df['age_binned'] = pd.cut(df['age'], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "# Verifico la distribución por grupos resultantes\n",
    "df['age_binned'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad99ff-5b78-40a9-bece-b6a6aef03769",
   "metadata": {},
   "source": [
    "<a id=\"section-four\"></a>\n",
    "<h2><strong>4 - INGENIERÍA DE CARACTERÍSTICAS</strong></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d408dcb-abec-497d-8c39-831662bc210d",
   "metadata": {},
   "source": [
    "### Ingeniería de Características\n",
    "\n",
    "En esta sección se generan las variables que serán utilizadas por los modelos de *machine learning*. La ingeniería de características consiste en transformar, crear o seleccionar atributos a partir del conjunto de datos original, con el objetivo de mejorar el rendimiento del modelo y capturar relaciones relevantes entre las variables.\n",
    "Estas transformaciones no solo buscan mejorar la precisión, sino también la interpretabilidad y eficiencia del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a9346-59e4-433f-9c03-4e806d022333",
   "metadata": {},
   "source": [
    "### Análisis de correlación entre variables numéricas relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8b508-1559-4760-b6b6-40b75da01564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de variables\n",
    "M = df[['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m',\n",
    "        'nr.employed', 'campaign', 'pdays', 'previous', 'y']]\n",
    "\n",
    "# Figura y máscara\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(M.corr(), dtype=bool))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(M.corr(), mask=mask, annot=True, cmap='coolwarm',\n",
    "            fmt=\".2f\", linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "\n",
    "# Ajustes para evitar recortes\n",
    "plt.title(\"Matriz de correlación entre variables numéricas seleccionadas\", fontsize=14, pad=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "ax.set_ylim(len(M.columns), 0)     #  Fuerzo que aparezca el eje Y completo\n",
    "plt.tight_layout()                 #  Ajuste general de espaciado\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd03b8-4c63-4dcc-88b8-2c7f209a3f10",
   "metadata": {},
   "source": [
    "### Análisis de correlación entre variables numéricas\n",
    "\n",
    "Se construyó una matriz de correlación para examinar la relación lineal entre las variables numéricas seleccionadas, incluyendo la variable objetivo `y`.\n",
    "\n",
    "A partir del heatmap generado, se pueden destacar los siguientes hallazgos relevantes:\n",
    "\n",
    "- **Alta correlación positiva** entre:\n",
    "  - `euribor3m` y `emp.var.rate` (**r = 0.97**)\n",
    "  - `euribor3m` y `nr.employed` (**r = 0.94**)\n",
    "  - `emp.var.rate` y `nr.employed` (**r = 0.91**)\n",
    "\n",
    "  Estas asociaciones indican una fuerte redundancia entre variables macroeconómicas, lo que podría justificar la eliminación de alguna de ellas o la aplicación de técnicas de reducción de dimensionalidad.\n",
    "\n",
    "- **Correlaciones con la variable objetivo `y`**:\n",
    "  - `euribor3m` (**r = -0.31**)\n",
    "  - `nr.employed` (**r = -0.35**)\n",
    "  - `emp.var.rate` (**r = -0.30**)\n",
    "  - `pdays` (**r = 0.27**)\n",
    "  - `previous` (**r = 0.23**)\n",
    "\n",
    "  Estas variables muestran correlaciones más relevantes con el resultado de la campaña (`y`), por lo que podrían tener valor predictivo en etapas posteriores del modelado.\n",
    "\n",
    "- **Variables como `campaign` y `cons.conf.idx` presentan baja correlación lineal** con el resto de las variables, lo que sugiere independencia lineal, aunque podrían aportar desde otras perspectivas no lineales.\n",
    "\n",
    "En conjunto, este análisis de correlación permite identificar tanto redundancias como posibles variables relevantes, lo que guiará decisiones de selección y transformación de atributos en las próximas etapas del proyecto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e40cd0-f38f-4b9f-91e0-069c49883645",
   "metadata": {},
   "source": [
    "<a id=\"section-four-subsection-one\"></a>\n",
    "### **4.1 Agrupaciones o transformaciones lógicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f413e07-3b53-4858-b9a2-4958d5279c79",
   "metadata": {},
   "source": [
    "### Agrupación de valores en variable job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a507145-c1b9-4e8d-b329-93d985d42638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupación de valores en variable job\n",
    "df[\"job\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783f48c-c951-4ef2-8fc4-11aae12947c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo un diccionario para agrupar las categorías de la variable 'job' en grupos más generales e interpretables.\n",
    "job_map = {\n",
    "    'admin.': 'White-collar',\n",
    "    'management': 'White-collar',\n",
    "    'technician': 'White-collar',\n",
    "    \n",
    "    'blue-collar': 'Blue-collar',\n",
    "    'services': 'Blue-collar',\n",
    "    'housemaid': 'Blue-collar',\n",
    "    \n",
    "    'entrepreneur': 'Self-employed',\n",
    "    'self-employed': 'Self-employed',\n",
    "    \n",
    "    'retired': 'Non-active',\n",
    "    'student': 'Non-active',\n",
    "    'unemployed': 'Non-active',\n",
    "    \n",
    "    'unknown': 'Other'\n",
    "}\n",
    "\n",
    "# Aplico el mapeo a la columna 'job' para crear una nueva variable agrupada: 'job_grouped'.\n",
    "df['job_grouped'] = df['job'].map(job_map)\n",
    "\n",
    "# Verifico las nuevas proporciones\n",
    "df['job_grouped'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190ca00-e434-47f5-8010-7bf77ca2e096",
   "metadata": {},
   "source": [
    "### Agrupación de valores en variable education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b727ec-4593-48d0-9f6e-2b4eb60ba08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupación de valores en variable education\n",
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643c129-c676-44b4-a6c8-8ea5fbe19fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al igual que con job, defino mapeo a categorías agrupadas\n",
    "education_map = {\n",
    "    'basic.9y': 'Basic',\n",
    "    'basic.4y': 'Basic',\n",
    "    'basic.6y': 'Basic',\n",
    "    'high.school': 'Middle',\n",
    "    'professional.course': 'Middle',\n",
    "    'university.degree': 'Superior',\n",
    "    'unknown': 'Other',\n",
    "    'illiterate': 'Other'\n",
    "}\n",
    "\n",
    "# Aplico la creación de la nueva variable agrupada\n",
    "df['education_grouped'] = df['education'].map(education_map)\n",
    "\n",
    "# Verifico el conteo\n",
    "df['education_grouped'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b45891-7789-4d14-8442-2e3c58e02d53",
   "metadata": {},
   "source": [
    "<a id=\"section-four-subsection-two\"></a>\n",
    "### **4.2 Generación de nuevas variables derivadas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678adc4-5d9c-41b9-af2c-5c647f82fe49",
   "metadata": {},
   "source": [
    "A partir de variables ya establecidas, procedo a generar algunas variables derivadas que puedan enriquecer y potenciar mi dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c800cb-b844-40df-af76-7b99a5b35d84",
   "metadata": {},
   "source": [
    "### Generación de variable para identificar existencia o no de contactos previos con el cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153434f-e82a-469f-aa12-32ccd261e78d",
   "metadata": {},
   "source": [
    "Habida cuenta que existe una alta presencia (mayoritaria incluso) de valores 0 en la variable 'previous'; decido generar una variable derivada.\n",
    "Esta nueva variable permitirá identificar si el cliente ha sido contactado previamente, al menos 1 vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73100167-50c5-4c7d-a1e1-7125743d60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de variable contacted_previously (contactado previamente). Si el cliente tuvo al menos 1 contacto previo su valor será 1, sino será 0\n",
    "df['contacted_previously'] = (df['previous'] >= 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d0565-9c4b-42fa-ac77-5d06190d256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contacted_previously'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9b13b-9702-4a3c-bfc7-f46795232a31",
   "metadata": {},
   "source": [
    "### Generación de variable para combinar simultáneamente la edad y estado civil del cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ebe52-85e9-4f24-866b-a0240ff5e502",
   "metadata": {},
   "source": [
    "En este caso genero una variable adicional que concatena los valores de 'age_binned' y 'marital' para así mostrar una mayor dimensionalidad del cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ea900-b1f9-4462-aaa0-dbc7549d1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo variable 'life_stage' que se obtiene al concatenar el segmento etario y el estado civil.\n",
    "df['life_stage'] = df['age_binned'].astype(str) + ' & ' + df['marital']\n",
    "#Cuento los valores resultantes para la nueva variable\n",
    "df['life_stage'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce0e95-01d2-4aee-9335-eeb6910cd229",
   "metadata": {},
   "source": [
    "### Generación de variable para combinar simultáneamente el empleo y nivel educativo del cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bae850-71cc-4391-beff-25faaf6471e2",
   "metadata": {},
   "source": [
    "En este caso genero una variable adicional que concatena los valores de 'job' y 'education' para así mostrar una mayor dimensionalidad del cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fbea3-8e8f-47f0-acbd-6c7dab1b852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo variable 'socio-economic' que se obtiene al concatenar el oficio/trabajo y el grado educativo obtenido.\n",
    "df['socio-economic'] = df['job'].astype(str) + ' & ' + df['education']\n",
    "#Cuento los valores resultantes para la nueva variable\n",
    "df['socio-economic'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fab337-c19f-4aea-913d-f113be4bdf88",
   "metadata": {},
   "source": [
    "<a id=\"section-four-subsection-three\"></a>\n",
    "### **4.3 Eliminación de variables redundantes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ff93a-82f5-431b-99fc-9e93b7a68ee3",
   "metadata": {},
   "source": [
    "En esta sección procedo a eliminar una de las variables macroeconómicas que observé que podríaser redundante durante el EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603bfc8-9287-4fdf-9e3f-b29844190cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de variable macroeconómica redundante\n",
    "df.drop(columns=['nr.employed'], inplace=True)\n",
    "\n",
    "# Verificación de que la columna fue efectivamente eliminada\n",
    "print(\"Columnas actuales en el DataFrame:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708a4f7-f899-451f-95e1-5d4a62d40b3f",
   "metadata": {},
   "source": [
    "### Eliminación de variable macroeconómica con baja relevancia o redundancia\n",
    "\n",
    "Como parte del proceso de selección de variables, se identificó y eliminó la siguiente variable macroeconómica del conjunto de datos:\n",
    "\n",
    "- `nr.employed`\n",
    "\n",
    "La decisión se basó en los siguientes criterios:\n",
    "\n",
    "1. **Redundancia por alta correlación entre variables**  \n",
    "   - `nr.employed` presentó una **alta correlación** con `emp.var.rate` (r = 0.91) y `euribor3m` (r = 0.94).\n",
    "   - Estas tres variables capturan información económica muy similar, lo cual puede generar problemas de **multicolinealidad**.\n",
    "   - Se optó por conservar `euribor3m`, que mostró la **mayor correlación con la variable objetivo `y`** y ofrece una interpretación macroeconómica relevante.\n",
    "\n",
    "\n",
    "2. **Interpretabilidad limitada**  \n",
    "   - Esta variable representa índices económicos agregados cuya relación directa con la decisión del cliente bancario es difusa.\n",
    "   - En un enfoque orientado a la explicación y claridad de resultados, se priorizaron variables más fácilmente interpretables.\n",
    "\n",
    "En consecuencia, se decidió eliminar esta variable para **reducir la dimensionalidad**, **minimizar redundancias** y **conservar únicamente aquellas variables con mayor relevancia estadística y práctica** para el análisis y modelado predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d4533-2b0f-4411-bed2-f6e8f838c025",
   "metadata": {},
   "source": [
    "<a id=\"section-four-subsection-four\"></a>\n",
    "### **4.4 Encoding de variables categóricas (OneHotEncoder)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285e76c-d159-4059-a473-d466ecf8e1bc",
   "metadata": {},
   "source": [
    "En esta sección se realiza la codificación **One-Hot Encoding** sobre las variables categóricas seleccionadas (`cat_cols1`). Esta técnica transforma cada categoría en una nueva columna binaria (0 o 1), permitiendo que los modelos de machine learning puedan procesar variables categóricas de manera adecuada.\n",
    "\n",
    "Finalmente, se eliminan las columnas categóricas originales y se agregan las columnas codificadas al DataFrame principal, manteniendo la correspondencia de índices para preservar la integridad de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2d8fe-d7b6-4827-9eda-771b32f1386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero obtengo las variables categóricas del dataframe\n",
    "cat_cols1= df.select_dtypes(include=['object','category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efdfab1-0c9b-4dc3-a96d-bbd7d068c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso cuáles variables se encuentran almacenadas en cat_cols1\n",
    "cat_cols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8dcfb-4078-420a-b630-4baec743670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observo los tipos de datos de cada variable, antes de efectuar el encoding con OneHotEncoder.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22b410-6872-4c9e-bdc5-a1de34ca4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializo OneHotEncoder con algunos parámetros clave:\n",
    "# - sparse_output=False: Devuelve un array denso (para mejor compatibilidad con pandas)\n",
    "# - handle_unknown='ignore': Permitirá manejar categorías no vistas durante el entrenamiento\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Realizo una conversión preventiva a string para evitar errores con tipos mixtos\n",
    "df[cat_cols1] = df[cat_cols1].astype(str)\n",
    "\n",
    "# Aplico el ajuste y transformación\n",
    "ohe_array = ohe.fit_transform(df[cat_cols1])\n",
    "ohe_columns = ohe.get_feature_names_out(cat_cols1)\n",
    "\n",
    "# Efectúo la reconstrucción del DataFrame:\n",
    "df_ohe = pd.DataFrame(ohe_array, columns=ohe_columns, index=df.index)# 1. Creo DF con las nuevas columnas one-hot (manteniendo el índice original)\n",
    "df.drop(columns=cat_cols1, inplace=True) # 2. Elimino columnas categóricas originales\n",
    "df = pd.concat([df, df_ohe], axis=1) # 3. Concateno con el DataFrame original\n",
    "\n",
    "# Hago el guardado del encoder para producción debido a que:\n",
    "# - Es útil para aplicar la misma transformación a nuevos datos\n",
    "# - El formato .pkl mantiene todos los parámetros aprendidos\n",
    "onehot_encoder = ohe\n",
    "joblib.dump(onehot_encoder, 'onehot_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d54822-9fc8-459e-a417-77df96f7a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso por última vez los tipos de datos de cada variable, luego de efectuar el encoding con OneHotEncoder.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94cdf9-8a86-4fff-acb9-54e1d168a448",
   "metadata": {},
   "source": [
    "<a id=\"section-five\"></a>\n",
    "<h2><strong>5 - MODELADO PREDICTIVO</strong></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cc69e-95d0-4dc5-b014-a5ae24a7747e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "Ahora que poseo los elementos necesarios para alimentar esta primera etapa de modelización de ML, genero una división de los datos estratificada según la variable objetivo, para luego pasarle al modelo y validar su eficiencia.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5ad5c-423b-4a02-9615-89b5a27d0bbe",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-one\"></a>\n",
    "### 5.1 - División del conjunto en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7c4c6-179a-4b1a-9c53-a19b3c4715ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divido el dataset en entrenamiento y prueba (80% train, 20% test) Y estratificado según variable objetivo\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('y',axis=1),\n",
    "                                                    df.y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify = df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc037e3-823e-4453-a3b7-d9f6a09fb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso cómo ha quedado la división X_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fa31c-42fe-4d0f-bba4-66288e9f279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso cómo ha quedado la división X_test\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1362040-7932-4ee8-bdc4-d7268e6bf881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso cómo ha quedado la división y_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69eac1e-e023-4b12-b2c9-7113fc63f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviso cómo ha quedado la división y_test\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8be43b-a6df-4ffe-b04b-89d92ced48e1",
   "metadata": {},
   "source": [
    "\n",
    "### Selección automática de features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e3ebb-cabd-4fb8-8152-d2732b28ab12",
   "metadata": {},
   "source": [
    "La selección de variables es una etapa clave en todo proceso de modelado predictivo, ya que permite reducir la dimensionalidad del conjunto de datos, eliminar redundancias y mejorar el rendimiento de los modelos. Para este trabajo se testearon distintos métodos de selección automática, tales como SelectKBest, importancia de variables basada en modelos (como Random Forest y XGBoost), y enfoques recursivos como RFECV (el cual se terminó seleccionando como el método con mejor rendimiento). \n",
    "Estas técnicas ayudan a identificar las características más relevantes que explican la variable objetivo, optimizando así tanto la capacidad predictiva como la interpretabilidad del modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f7cf2-bae5-4fe0-ba8a-149b595365eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vuelvo a generar un heatmap (mapa de calor) de las correlaciones entre las variables numéricas del DataFrame previo\n",
    "#Esto(ahora que poseo todas las variables como numéricas) puede permitirme analizar las posiblidades sobre selección de variables.\n",
    "%matplotlib inline\n",
    "df_correlacion = df.select_dtypes(include=['number'])\n",
    "correlation_mat = df_correlacion.corr()\n",
    "sns.heatmap(correlation_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef22ff-a991-4290-8d64-5a8665277c29",
   "metadata": {},
   "source": [
    "Del análisis del mapa de calor de correlaciones se puede aventurar que no se identifican pares de variables con alta correlación (por ejemplo, |r| > 0.85) que justifiquen su eliminación. En general, las correlaciones entre variables numéricas son bajas, lo que indica que cada atributo aporta información relativamente independiente. Esta observación respaldaría la inclusión de una cantidad mayor de variables a las esperadas en etapas posteriores del modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4b396-a20b-4c80-aef4-4589e7d7b9ee",
   "metadata": {},
   "source": [
    "A continuación y para optimizar el conjunto de variables utilizadas en el modelo, se aplicó el método Recursive Feature Elimination with Cross-Validation (RFECV) utilizando un clasificador XGBoost como estimador base. Esta técnica evalúa recursivamente la importancia de las variables, eliminando en cada iteración la menos relevante, y valida el desempeño a través de validación cruzada estratificada con 5 particiones. Como métrica de evaluación se empleó el F1-score, debido al desbalance de clases presente en el dataset. El proceso permitió identificar el subconjunto de variables más relevantes para la tarea de clasificación, lo que contribuye a reducir la complejidad del modelo, mejorar su interpretabilidad y potencialmente mitigar el riesgo de sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f72814-8d5d-46d7-b3c1-d7b1e8479786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empleo modelo base con XGBoost\n",
    "modelo_xgb = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=7.85  \n",
    ")\n",
    "\n",
    "# Realizo la validación cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Ejecuto RFECV\n",
    "selector_rfecv = RFECV(\n",
    "    estimator=modelo_xgb,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "selector_rfecv.fit(X_train, y_train)\n",
    "\n",
    "# Obtengo las features seleccionadas\n",
    "selected_features_rfecv = X_train.columns[selector_rfecv.support_].tolist()\n",
    "print(f\"Se seleccionaron {len(selected_features_rfecv)} variables:\")\n",
    "print(selected_features_rfecv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484dd2b3-7729-4fe7-ac29-71b583d5d0fd",
   "metadata": {},
   "source": [
    "Como resultado, el algoritmo seleccionó un total de **108 variables** de un total inicial de 162.\n",
    "\n",
    "Entre las variables seleccionadas se incluyen:\n",
    "- Variables originales: `age`, `month`, `duration`, `emp.var.rate`, etc.\n",
    "- Variables transformadas y codificadas: `job_admin.`, `housing_yes`, `poutcome_success`, etc.\n",
    "- Variables creadas durante el preprocesamiento: `age_binned_middle aged`, `job_grouped_White-collar`, `socio-economic_management & university.degree`, entre otras.\n",
    "\n",
    "Estas variables seleccionadas se utilizaron en el pipeline final para entrenar el modelo definitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397e000-43fc-4bba-ba61-36c195f81488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo la lista de features seleccionadas para un potencial uso posterior\n",
    "joblib.dump(selected_features_rfecv, 'selected_features_rfecv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d563d-4540-4242-aaf1-c73e009fe559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplico las mismas características seleccionadas al conjunto de train y test para mantener la consistencia en el espacio de características\n",
    "X_train = X_train[selected_features_rfecv] # Filtro solo las features seleccionadas en el train\n",
    "X_test = X_test[selected_features_rfecv] # Aplico el mismo filtro al test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec53cd-3b96-488f-80e0-33fc98fc719c",
   "metadata": {},
   "source": [
    "### Escalado de los datos\n",
    "\n",
    "El escalado es una etapa clave del preprocesamiento, especialmente cuando se utilizan modelos sensibles a la magnitud de las variables. En este TFM, las variables numéricas como la duración de llamada, la edad o el número de contactos presentan distintas escalas, lo que podría sesgar el modelo.\n",
    "\n",
    "Para garantizar que cada variable contribuya equitativamente al aprendizaje, se aplica `MinMaxScaler`, que normaliza los valores en un rango de 0 a 1. Esto mejora la estabilidad del modelo, acelera la convergencia y evita que variables con mayor rango dominen el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae7ce0-57fb-4859-9d3f-869e0dd07d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializo el escalador MinMax para normalizar los datos al rango [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Guardo los índices y columnas originales\n",
    "columns = X_train.columns # Nombres originales de las features\n",
    "index_train = X_train.index # Índices del conjunto de entrenamiento\n",
    "index_test = X_test.index # Índices del conjunto de prueba\n",
    "\n",
    "# Escalo y reconstruyo los DataFrames con nombres\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=columns, index=index_train)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=columns, index=index_test)\n",
    "\n",
    "# Guardo el scaler en formato pkl\n",
    "joblib.dump(scaler, 'minmax_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad254c-9c7b-4556-b0b8-b2531b8d74c0",
   "metadata": {},
   "source": [
    "### Balanceo de los datos\n",
    "\n",
    "El dataset presenta un fuerte desbalance entre las clases: la mayoría de los clientes no contrata el depósito, mientras que solo un pequeño porcentaje sí lo hace. Esta desproporción puede sesgar al modelo hacia la clase mayoritaria, afectando negativamente su capacidad para identificar correctamente los casos positivos.\n",
    "\n",
    "Para abordar este problema, se aplica la técnica de sobremuestreo **SMOTE (Synthetic Minority Over-sampling Technique)**, que genera ejemplos sintéticos de la clase minoritaria a partir de sus vecinos más cercanos. De este modo, se obtiene un conjunto de entrenamiento más equilibrado, mejorando la capacidad del modelo para aprender patrones representativos y detectar clientes potenciales de manera más efectiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09cb55-3ce4-45f7-9c17-b470596f2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b141a54-69cf-4b80-b200-89d549cb3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de distribución de clases antes del balanceo\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) # Clase minoritaria\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))  # Clase mayoritaria\n",
    "\n",
    "# Aplico SMOTE solo al conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42) # random_state para reproducibilidad\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train) # Solo aplicamos al conjunto de entrenamiento\n",
    "\n",
    "# Verificación de resultados post-aplicación\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n",
    "\n",
    "# Confirmación de balanceo perfecto (50-50)\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea36de-dba7-4f70-aae4-3382a1278700",
   "metadata": {},
   "source": [
    "**Conclusión del balanceo con SMOTE**\n",
    "\n",
    "Antes del balanceo, el conjunto de entrenamiento presentaba un fuerte desbalance: solo el 11% de las instancias correspondían a la clase positiva (`y = 1`). Esta desproporción podía sesgar al modelo a favor de la clase mayoritaria, reduciendo su capacidad de detectar correctamente clientes que aceptan la oferta bancaria.\n",
    "\n",
    "Tras aplicar **SMOTE**, se obtuvo un conjunto de entrenamiento perfectamente balanceado, con igual cantidad de instancias para ambas clases (29.229 cada una). Esto permite entrenar modelos más justos y sensibles a la clase minoritaria, mejorando la capacidad predictiva en escenarios desbalanceados como el de este TFM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d72b9-51fe-4704-a61b-aa6dbcdd064b",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-two\"></a>\n",
    "### 5.2 - Elección de métricas: accuracy, precision, recall, F1, AUC\n",
    "\n",
    "El objetivo del modelo es ayudar a identificar qué clientes tienen mayor probabilidad de contratar un depósito a plazo. En este tipo de problemas, donde la mayoría de los clientes no contrata el producto, medir solo el porcentaje de aciertos (*accuracy*) puede dar una visión incompleta.\n",
    "\n",
    "Por eso se utilizan métricas adicionales que permiten evaluar mejor el impacto real del modelo:\n",
    "- **Precision**: ayuda a entender cuántas de las predicciones positivas fueron realmente acertadas (importante para no malgastar recursos en clientes poco interesados).\n",
    "- **Recall**: indica cuántos clientes realmente interesados fueron correctamente identificados (clave para no perder oportunidades comerciales).\n",
    "- **F1-Score**: equilibra ambos aspectos.\n",
    "- **AUC**: permite comparar modelos considerando su capacidad general de diferenciación entre quienes contratarían o no.\n",
    "\n",
    "Estas métricas ofrecen una visión más útil para la toma de decisiones comerciales y el diseño de campañas efectivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b812c-5666-43ac-8221-ad581d7868e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que imprime métricas de evaluación de un modelo de clasificación\n",
    "# Acepta:\n",
    "#   y_true  -> etiquetas reales\n",
    "#   y_pred  -> predicciones del modelo (clase 0 o 1)\n",
    "#   y_proba -> probabilidades predichas (necesarias para AUC y curva ROC)\n",
    "def saca_metricas(y_true, y_pred, y_proba=None):\n",
    "    print('🔹 Matriz de Confusión:')\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    print('\\n🔹 Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, target_names=['No', 'Yes']))\n",
    "    \n",
    "    print('🔹 Accuracy:', round(accuracy_score(y_true, y_pred), 4))\n",
    "    print('🔹 Precision:', round(precision_score(y_true, y_pred, pos_label=1), 4))\n",
    "    print('🔹 Recall:', round(recall_score(y_true, y_pred, pos_label=1), 4))\n",
    "    print('🔹 F1 Score:', round(f1_score(y_true, y_pred, pos_label=1), 4))\n",
    "    \n",
    "    # Curva ROC\n",
    "    if y_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_proba, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print('🔹 AUC:', round(roc_auc, 4))\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='Curva ROC', line=dict(color='blue')))\n",
    "        fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Línea base', line=dict(color='red', dash='dash')))\n",
    "        fig.update_layout(\n",
    "            title=f'Curva ROC (AUC = {roc_auc:.2f})',\n",
    "            xaxis_title='Tasa de Falsos Positivos',\n",
    "            yaxis_title='Tasa de Verdaderos Positivos (Recall)',\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        fig.show()\n",
    "    else:\n",
    "        print('No se calculó la curva ROC porque no se pasó `y_proba` (probabilidades).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4031b2-56b5-44d9-9b51-3dd702a24bc0",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-three\"></a>\n",
    "### 5.3 - Definición del baseline\n",
    "\n",
    "Para evaluar el desempeño del modelo y garantizar su capacidad de generalización, se aplica validación cruzada *k-fold* durante el entrenamiento.\n",
    "\n",
    "Además, se define un **modelo baseline** utilizando un clasificador dummy con la estrategia `most_frequent`, que siempre predice la clase mayoritaria. Esto sirve como referencia mínima para comparar y validar que los modelos entrenados aportan un valor real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb547b-9355-4b85-b802-a435d510a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo baseline que siempre predice la clase mayoritaria\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "# Entrenamiento del baseline con los datos de entrenamiento\n",
    "dummy.fit(X_train, y_train)\n",
    "# Predicción de etiquetas para el conjunto de prueba\n",
    "y_pred_dummy = dummy.predict(X_test)\n",
    "# Predicción de probabilidades para la clase positiva (usada para métricas como AUC)\n",
    "y_proba_dummy = dummy.predict_proba(X_test)[:, 1]\n",
    "# Cálculo y visualización de métricas de evaluación\n",
    "saca_metricas(y_test, y_pred_dummy, y_proba_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749014b5-fb24-4731-a57d-d04ff61a141a",
   "metadata": {},
   "source": [
    "### Resultados del modelo baseline\n",
    "\n",
    "Después de entrenar el clasificador dummy con la estrategia `most_frequent`, que siempre predice la clase mayoritaria (\"No\"), obtenemos los siguientes resultados:\n",
    "\n",
    "- La matriz de confusión muestra que el modelo clasifica correctamente todos los casos negativos (7308), pero no detecta ningún caso positivo (928), lo que refleja un sesgo hacia la clase mayoritaria.\n",
    "- El reporte de clasificación indica una precisión, recall y F1-score nulos para la clase minoritaria (\"Yes\"), confirmando que el modelo no es capaz de identificar clientes que sí suscriben al depósito.\n",
    "- La exactitud general (accuracy) es alta (aprox. 88.7%) debido al desbalance del dataset, pero métricas como el AUC (0.5) y el F1 Score evidencian la ineficacia del baseline para predecir correctamente la clase positiva.\n",
    "\n",
    "Estos resultados resaltan la importancia de desarrollar modelos más sofisticados que superen este baseline y sean capaces de identificar patrones significativos para predecir la suscripción con mayor precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e2354-12d4-4c3d-a524-539716dd64fd",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-four\"></a>\n",
    "### 5.4 - Modelo de Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d219d-7bde-44b5-b7bb-8ceb151708fa",
   "metadata": {},
   "source": [
    "En esta sección se entrena y evalúa un **modelo de regresión logística** como una primera aproximación real al problema de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822b798-ff00-4941-82db-b73e8ee7353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo con random_state para asegurar reproducibilidad\n",
    "modelo_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Configuración de la validación cruzada estratificada con 5 particiones\n",
    "# Se mantiene la proporción de clases en cada fold y se añade aleatoriedad con shuffle\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluación del modelo con validación cruzada utilizando la métrica F1 (adecuada para clases desbalanceadas)\n",
    "scores = cross_val_score(modelo_lr, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"F1 promedio con validación cruzada:\", scores.mean())\n",
    "\n",
    "# Entreno en todo el train para evaluar en test (final)\n",
    "modelo_lr.fit(X_train, y_train)\n",
    "y_pred_lr = modelo_lr.predict(X_test)\n",
    "y_proba = modelo_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluación del modelo sobre el conjunto de test y grafico las métricas correspondientes a este modelo\n",
    "saca_metricas(y_test, y_pred_lr, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef39858-bed7-4e41-bee4-d7943865d252",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo de Regresión Logística\n",
    "\n",
    "A continuación se presentan las observaciones obtenidas a partir del desempeño del modelo de regresión logística.\n",
    "\n",
    "---\n",
    "\n",
    "#### Observaciones clave\n",
    "\n",
    "1. **Excelente capacidad de discriminación global**\n",
    "   - El valor de **AUC = 0.93** indica que el modelo tiene una muy buena capacidad para distinguir entre clientes que aceptan y los que no aceptan la oferta.\n",
    "   - La curva ROC se sitúa claramente por encima de la línea base, lo que valida su buen comportamiento como clasificador binario.\n",
    "\n",
    "2. **F1 Score elevado en validación cruzada**\n",
    "   - Se obtuvo un **F1 promedio de 0.88** con validación cruzada estratificada, lo que sugiere un rendimiento consistente del modelo durante el entrenamiento.\n",
    "\n",
    "3. **Desempeño por clase: fuerte en recall, débil en precisión para la clase positiva**\n",
    "   - Para la clase **\"Yes\" (positiva)**:\n",
    "     - **Precision**: 0.45 → de todas las predicciones positivas, solo el 45% fueron correctas.\n",
    "     - **Recall**: 0.85 → el modelo logra capturar correctamente el 85% de los casos verdaderamente positivos.\n",
    "     - **F1 Score**: 0.59 → balancea ambos extremos, pero se ve penalizado por la baja precisión.\n",
    "\n",
    "4. **Alta cantidad de verdaderos positivos, pero también muchos falsos positivos**\n",
    "   - La **matriz de confusión** indica:\n",
    "     - Verdaderos positivos: 793\n",
    "     - Falsos positivos: 970\n",
    "   - Esto indica una alta sensibilidad, pero con muchos clientes clasificados erróneamente como potenciales positivos.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones\n",
    "\n",
    "- El modelo muestra un **muy buen rendimiento general**, especialmente en la capacidad de identificar clientes que aceptan la oferta (`recall` alto).\n",
    "- Sin embargo, genera una cantidad considerable de **falsos positivos**, lo que reduce la **precisión** y puede traducirse en campañas ineficientes o molestas para los clientes.\n",
    "\n",
    "El análisis sugiere que la regresión logística es un modelo competitivo y útil como punto de partida, aunque probablemente mejorable con enfoques no lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf1363-30d7-416a-bb83-69ef0a4a8c2d",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-five\"></a>\n",
    "### 5.5 - Modelo de Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a58acc-a59e-4fad-ad0f-be8a0593a7aa",
   "metadata": {},
   "source": [
    "Luego y en esta sección se entrena y evalúa un modelo de **Árbol de decisión**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557605e5-abc0-4e1e-935f-b0ee5948bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo con random_state para asegurar reproducibilidad\n",
    "modelo_dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Configuración de la validación cruzada estratificada con 5 particiones\n",
    "# Se mantiene la proporción de clases en cada fold y se añade aleatoriedad con shuffle\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluación del modelo con validación cruzada utilizando la métrica F1 (adecuada para clases desbalanceadas)\n",
    "scores_dt = cross_val_score(modelo_dt, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"F1 promedio con validación cruzada (Árbol de Decisión):\", round(scores_dt.mean(), 4))\n",
    "\n",
    "# Entrenamiento en todo el conjunto de entrenamiento\n",
    "modelo_dt.fit(X_train, y_train)\n",
    "\n",
    "# Predicción sobre conjunto de prueba\n",
    "y_pred_dt = modelo_dt.predict(X_test)\n",
    "y_proba_dt = modelo_dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluación del modelo sobre el conjunto de test y grafico las métricas correspondientes a este modelo\n",
    "saca_metricas(y_test, y_pred_dt, y_proba_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc583f56-91c7-4bcc-8e3e-e05b207b8ff4",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo de Árbol de Decisión\n",
    "\n",
    "A continuación se presentan las observaciones obtenidas a partir del desempeño del modelo de árbol de decisión.\n",
    "\n",
    "---\n",
    "\n",
    "#### Observaciones clave\n",
    "\n",
    "1. **Rendimiento aceptable en discriminación global**\n",
    "   - El valor de **AUC = 0.75** indica que el modelo tiene una capacidad **moderada** para distinguir entre clientes que aceptan y no aceptan la oferta.\n",
    "   - La curva ROC se sitúa por encima de la línea base, aunque no tan alejada como en otros modelos más robustos (como la regresión logística o modelos de ensamblado).\n",
    "\n",
    "2. **F1 promedio alto en validación cruzada**\n",
    "   - Se obtuvo un **F1 promedio de 0.9283**, lo que sugiere un **buen ajuste durante el entrenamiento** en los distintos folds.\n",
    "   - Sin embargo, al evaluar en el conjunto de prueba, el desempeño baja, lo que puede estar indicando **sobreajuste** (overfitting), fenómeno común en árboles sin poda o sin regularización.\n",
    "\n",
    "3. **Desempeño por clase: balance discreto, con mejores resultados en la clase mayoritaria**\n",
    "   - Para la clase **\"Yes\" (positiva)**:\n",
    "     - **Precision**: 0.51 → algo mejor que la regresión logística.\n",
    "     - **Recall**: 0.58 → captura solo el 58% de los positivos reales.\n",
    "     - **F1 Score**: 0.54 → indica un rendimiento moderado, penalizado por el bajo recall.\n",
    "   - Para la clase **\"No\" (negativa)**:\n",
    "     - Alto desempeño con un F1 de 0.94, reflejo del desbalance de clases y de la facilidad para identificar los negativos.\n",
    "\n",
    "4. **Mejor precisión que la regresión logística, pero menor capacidad de cobertura de positivos**\n",
    "   - El árbol de decisión logra **menos falsos positivos** (mejor precisión), pero a costa de **no identificar correctamente tantos casos positivos reales**.\n",
    "   - Esto puede observarse en la **matriz de confusión**:\n",
    "     - Verdaderos positivos: 539\n",
    "     - Falsos positivos: 520\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones\n",
    "\n",
    "- El modelo de árbol de decisión ofrece un **buen rendimiento general**, con una **precisión ligeramente superior** en la clase positiva respecto al modelo de regresión logística.\n",
    "- No obstante, su menor **recall (0.58)** implica que **deja pasar una parte considerable de clientes que sí aceptarían la oferta**, lo que podría representar oportunidades de negocio perdidas.\n",
    "- El valor de **AUC = 0.75** lo posiciona por debajo de otros modelos más sofisticados, lo que indica un margen importante de mejora.\n",
    "- Además, el posible **sobreajuste** observado entre entrenamiento y test sugiere que sería recomendable aplicar técnicas de **poda, ajuste de hiperparámetros o incluso optar por modelos de ensamblado** como Random Forest o XGBoost para mejorar su capacidad generalizadora.\n",
    "\n",
    "En resumen, el árbol de decisión resulta útil como modelo interpretable y de bajo costo computacional, pero presenta limitaciones que podrían abordarse con enfoques más complejos o regularizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde5c4e-4e89-4478-a93a-10268d8cf2b3",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-six\"></a>\n",
    "### 5.6 - Modelo de Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb409b0c-3e80-43a9-9376-65db6f8d47a1",
   "metadata": {},
   "source": [
    "Continúo ahora por ajustar y entrenar un ***modelo de Random Forest***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa89cb-bc8f-4c69-9291-59ef54a70ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo con random_state para asegurar reproducibilidad\n",
    "modelo_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Configuración de la validación cruzada estratificada con 5 particiones\n",
    "# Se mantiene la proporción de clases en cada fold y se añade aleatoriedad con shuffle\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluación del modelo con validación cruzada utilizando la métrica F1 (adecuada para clases desbalanceadas)\n",
    "scores_rf = cross_val_score(modelo_rf, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"F1 promedio con validación cruzada (Random Forest):\", round(scores_rf.mean(), 4))\n",
    "\n",
    "# Entreno el modelo en todo el conjunto de entrenamiento\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicción sobre el conjunto de test\n",
    "y_pred_rf = modelo_rf.predict(X_test)\n",
    "y_proba_rf = modelo_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluación del modelo sobre el conjunto de test y grafico las métricas correspondientes a este modelo\n",
    "saca_metricas(y_test, y_pred_rf, y_proba_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a113478-d6a8-4834-a4cd-ed404b0048fc",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo Random Forest\n",
    "\n",
    "A continuación se presentan las observaciones obtenidas a partir del desempeño del modelo Random Forest aplicado al conjunto de test, complementado con validación cruzada.\n",
    "\n",
    "### Observaciones clave\n",
    "\n",
    "#### Excelente capacidad de discriminación global\n",
    "- **AUC = 0.94**, lo cual indica una capacidad sobresaliente para distinguir entre clientes que aceptan y los que no aceptan la oferta.\n",
    "- La **curva ROC** se sitúa muy por encima de la línea base, reforzando su eficacia como clasificador binario.\n",
    "\n",
    "#### F1 Score alto en validación cruzada\n",
    "- Se obtuvo un **F1 promedio de 0.9566** con validación cruzada estratificada, lo que sugiere un rendimiento muy sólido y consistente del modelo en los diferentes folds del entrenamiento.\n",
    "\n",
    "#### Desempeño por clase: fuerte en clase negativa, limitado en clase positiva\n",
    "- Para la clase **\"Yes\" (positiva)**:\n",
    "  - **Precision: 0.61** → De todas las predicciones positivas, solo el 61% fueron correctas.\n",
    "  - **Recall: 0.56** → El modelo logra capturar el 56% de los verdaderos positivos.\n",
    "  - **F1 Score: 0.58** → Moderado, penalizado por el balance entre precisión y recall.\n",
    "\n",
    "- Para la clase **\"No\" (negativa)**:\n",
    "  - El desempeño es notable, con una precisión y recall de aproximadamente **0.94–0.95**.\n",
    "\n",
    "#### Menor proporción de falsos positivos respecto a Regresión Logística\n",
    "- Según la matriz de confusión:\n",
    "  - **Verdaderos positivos: 519**\n",
    "  - **Falsos positivos: 331**\n",
    "- Esto representa una mejora considerable en la precisión del modelo respecto al baseline logístico, con menor cantidad de clientes mal clasificados como positivos.\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "- El modelo **Random Forest mejora claramente la precisión y el AUC** respecto a la regresión logística, lo cual es esperable dada su naturaleza no lineal y capacidad para modelar relaciones complejas.\n",
    "- Sin embargo, **el recall de la clase positiva se reduce**, lo que implica que el modelo podría estar dejando pasar algunos clientes que aceptarían la oferta.\n",
    "- Puede ser un buen modelo si se desea **minimizar falsos positivos** y tener un clasificador más conservador.\n",
    "- Como estrategia complementaria, podría explorarse un ajuste del **umbral de decisión**, o un **modelo de ensamblado** que combine Random Forest con otro algoritmo con mayor sensibilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28a38a-6406-4a7b-bfa2-d5bf1117bbfe",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-seven\"></a>\n",
    "### 5.7 - Modelo de XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303a749-0279-4887-b959-27bc7109797b",
   "metadata": {},
   "source": [
    "En esta parte probaré el uso de un ***modelo XGBoost***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c252da-2a52-48ee-87b3-08bb4f44d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo con random_state para asegurar reproducibilidad\n",
    "modelo_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Configuración de la validación cruzada estratificada con 5 particiones\n",
    "# Se mantiene la proporción de clases en cada fold y se añade aleatoriedad con shuffle\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluación del modelo con validación cruzada utilizando la métrica F1 (adecuada para clases desbalanceadas)\n",
    "scores_xgb = cross_val_score(modelo_xgb, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"F1 promedio con validación cruzada (XGBoost):\", round(scores_xgb.mean(), 4))\n",
    "\n",
    "# Entreno el modelo con todos los datos de entrenamiento\n",
    "modelo_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones sobre conjunto de test\n",
    "y_pred_xgb = modelo_xgb.predict(X_test)\n",
    "y_proba_xgb = modelo_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluación del modelo sobre el conjunto de test y grafico las métricas correspondientes a este modelo\n",
    "saca_metricas(y_test, y_pred_xgb, y_proba_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236b5a3-8f5e-40b7-b648-16209948b2c6",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo XGBoost\n",
    "\n",
    "A continuación se detallan las observaciones clave obtenidas a partir del desempeño del modelo XGBoost aplicado al conjunto de test.\n",
    "\n",
    "### Observaciones clave\n",
    "\n",
    "#### Excelente capacidad de discriminación global\n",
    "- El valor **AUC = 0.95** indica que el modelo posee una **capacidad sobresaliente para distinguir** entre clientes que aceptan y los que no aceptan la oferta.\n",
    "- La curva ROC se mantiene muy por encima de la línea base, lo que **valida la eficacia del modelo como clasificador binario**.\n",
    "\n",
    "#### F1 Score elevado en validación cruzada\n",
    "- Se obtuvo un **F1 promedio de 0.951** con validación cruzada estratificada, lo que sugiere un **rendimiento consistente y robusto** del modelo durante el entrenamiento.\n",
    "\n",
    "#### Desempeño mixto por clase: fuerte en la clase negativa, débil en la clase positiva\n",
    "- Para la clase **\"Yes\"** (positiva):\n",
    "  - **Precision: 0.63** → de todas las predicciones positivas, solo el 63% fueron correctas.\n",
    "  - **Recall: 0.59** → el modelo identifica correctamente el 59% de los clientes que efectivamente aceptaron la oferta.\n",
    "  - **F1 Score: 0.61** → se logra un equilibrio razonable entre precisión y recall, aunque aún lejos de ser óptimo.\n",
    "\n",
    "- Para la clase **\"No\"**:\n",
    "  - Alto rendimiento, con una F1-score de **0.95**, reflejando la facilidad del modelo para detectar correctamente a quienes no aceptan la oferta.\n",
    "\n",
    "#### Reducción en falsos positivos respecto a la regresión logística\n",
    "- Matriz de confusión:\n",
    "  - **Verdaderos positivos (VP): 546**\n",
    "  - **Falsos positivos (FP): 325**\n",
    "- En comparación con la regresión logística (970 FP), XGBoost **reduce considerablemente los falsos positivos**, lo que implica **menos interferencias comerciales innecesarias**.\n",
    "\n",
    "#### Buen rendimiento global en métricas generales\n",
    "- **Accuracy: 0.9142** → más del 91% de las predicciones fueron correctas.\n",
    "- **Precision global: 0.6269**\n",
    "- **Recall global: 0.5884**\n",
    "- **F1 global: 0.607**\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "- El modelo **XGBoost supera a la regresión logística** en métricas clave como **AUC, F1 en validación cruzada** y especialmente en la **reducción de falsos positivos**.\n",
    "- Aunque aún queda margen de mejora en la **precisión y recall para la clase positiva**, se observa un **mayor equilibrio y eficiencia general**.\n",
    "- Su buen rendimiento general, junto con su bajo nivel de sobreajuste, lo convierten en una **excelente alternativa para la predicción de aceptación de campañas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c9ef1-a04d-43c6-8e44-036d6f3d6f84",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-eight\"></a>\n",
    "### 5.8 - Modelo de K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dc0a1-4ed2-4c5f-b12a-e9f2ec886de8",
   "metadata": {},
   "source": [
    "En esta sub-sección testeo una solución basada en ***K-Nearest Neighbors***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5860e2c-28de-4247-88f7-b398721f44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo\n",
    "modelo_knn = KNeighborsClassifier()\n",
    "\n",
    "# Configuración de la validación cruzada estratificada con 5 particiones\n",
    "# Se mantiene la proporción de clases en cada fold y se añade aleatoriedad con shuffle\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluación del modelo con validación cruzada utilizando la métrica F1 (adecuada para clases desbalanceadas)\n",
    "scores_knn = cross_val_score(modelo_knn, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"F1 promedio con validación cruzada (KNN):\", round(scores_knn.mean(), 4))\n",
    "\n",
    "# Entrenamiento final\n",
    "modelo_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_knn = modelo_knn.predict(X_test)\n",
    "y_proba_knn = modelo_knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluación del modelo sobre el conjunto de test y grafico las métricas correspondientes a este modelo\n",
    "saca_metricas(y_test, y_pred_knn, y_proba_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687f019-a8d1-4616-8cc0-e801bbda04ec",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo K-Nearest Neighbors (KNN)\n",
    "\n",
    "A continuación se presentan las observaciones obtenidas a partir del desempeño del modelo de K-Nearest Neighbors.\n",
    "\n",
    "### Observaciones clave\n",
    "\n",
    "- **Capacidad de discriminación razonable**\n",
    "  - El valor de **AUC = 0.74** sugiere que el modelo tiene una capacidad moderada para distinguir entre clientes que aceptan y los que no aceptan la oferta.\n",
    "  - La **curva ROC** se sitúa por encima de la línea base, aunque no de forma sobresaliente, lo que indica un desempeño razonable como clasificador binario, pero lejos de óptimo.\n",
    "\n",
    "- **F1 Score alto en validación cruzada**\n",
    "  - Se obtuvo un **F1 promedio de 0.8872** usando validación cruzada estratificada, lo cual sugiere un rendimiento robusto durante el entrenamiento, especialmente considerando el desbalance de clases.\n",
    "\n",
    "- **Desempeño por clase: buen recall, baja precisión para la clase positiva**\n",
    "  - Para la clase **\"Yes\"** (cliente que acepta la oferta):\n",
    "    - **Precision: 0.30** → Solo el 30% de las predicciones positivas fueron correctas.\n",
    "    - **Recall: 0.55** → El modelo logra capturar correctamente el 55% de los casos verdaderamente positivos.\n",
    "    - **F1 Score: 0.39** → Se ve penalizado por la baja precisión, aunque se beneficia del recall.\n",
    "\n",
    "- **Matriz de confusión: muchos falsos positivos**\n",
    "  - Verdaderos negativos: 6090  \n",
    "  - Falsos positivos: 1218  \n",
    "  - Falsos negativos: 413  \n",
    "  - Verdaderos positivos: 515  \n",
    "  - Esto muestra un número significativo de **falsos positivos**, lo cual podría generar costos o esfuerzos innecesarios en campañas dirigidas.\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "- El modelo KNN muestra un desempeño aceptable en cuanto a recall, logrando identificar más de la mitad de los clientes interesados.\n",
    "- No obstante, su **precisión es baja**, lo que indica que muchas predicciones positivas resultan ser incorrectas.\n",
    "- El valor de AUC cercano a 0.74 confirma que se trata de un modelo **mejor que el azar**, aunque **menos eficaz que otros modelos más sofisticados**.\n",
    "- En contextos donde es importante reducir falsos positivos (por ejemplo, para minimizar costos de campañas o evitar molestar clientes), **KNN podría no ser la mejor opción sin algún tipo de ajuste adicional** (por ejemplo, optimización de hiperparámetros o técnicas de balanceo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769eb6eb-6486-4ead-83a1-e7920e6b2898",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-nine\"></a>\n",
    "### 5.9 - Modelo de Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c720af-9631-478b-a899-f8854d8d80e8",
   "metadata": {},
   "source": [
    "Ahora, procedo a probar el desempeño con un ***modelo Naive Bayes***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e263a32-d82c-4c5a-bb08-89fb1c523026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Naive Bayes\n",
    "modelo_nb = GaussianNB()\n",
    "\n",
    "# Configuración de la validación cruzada estratificada con 5 particiones\n",
    "# Se mantiene la proporción de clases en cada fold y se añade aleatoriedad con shuffle\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluación del modelo con validación cruzada utilizando la métrica F1 (adecuada para clases desbalanceadas)\n",
    "scores_nb = cross_val_score(modelo_nb, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "print(\"F1 promedio con validación cruzada (Naive Bayes):\", round(scores_nb.mean(), 4))\n",
    "\n",
    "# Entreno el modelo final\n",
    "modelo_nb.fit(X_train, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred_nb = modelo_nb.predict(X_test)\n",
    "y_proba_nb = modelo_nb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluación del modelo sobre el conjunto de test y grafico las métricas correspondientes a este modelo\n",
    "saca_metricas(y_test, y_pred_nb, y_proba_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d645f29-0057-4d26-bca3-e99bd96c42e9",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo Naive Bayes\n",
    "\n",
    "A continuación se presentan las observaciones obtenidas a partir del desempeño del modelo Naive Bayes aplicado al conjunto de datos de marketing bancario.\n",
    "\n",
    "### Observaciones clave\n",
    "\n",
    "#### Buena capacidad general de discriminación\n",
    "- El valor de AUC = **0.78** indica que el modelo posee una **buena capacidad para distinguir** entre clientes que aceptan y los que no aceptan la oferta.\n",
    "- La **curva ROC** se encuentra bien por encima de la línea base aleatoria, mostrando un rendimiento clasificatorio aceptable, aunque inferior al de otros modelos más complejos como regresión logística.\n",
    "\n",
    "#### F1 Score razonable con validación cruzada\n",
    "- Se obtuvo un **F1 promedio de 0.72** utilizando validación cruzada estratificada, lo cual sugiere una **consistencia adecuada durante el entrenamiento** a pesar de tratarse de un modelo simple y lineal.\n",
    "\n",
    "#### Desempeño por clase: excelente recall, baja precisión para la clase positiva\n",
    "- Para la clase **\"Yes\"** (respuesta positiva del cliente):\n",
    "  - **Precision: 0.26** → el modelo clasifica como positivos muchos casos incorrectos, es decir, **alta tasa de falsos positivos**.\n",
    "  - **Recall: 0.71** → logra capturar correctamente una gran proporción de los clientes que realmente aceptaron la oferta.\n",
    "  - **F1 Score: 0.38** → penalizado por la baja precisión, aunque mejora por el buen recall.\n",
    "\n",
    "#### Matriz de confusión: fuerte sensibilidad, pero con costo de precisión\n",
    "- Verdaderos positivos: **656**\n",
    "- Falsos positivos: **1831**\n",
    "- Esto refuerza que el modelo **prioriza la detección de positivos** (alta sensibilidad), aunque incurre en muchos errores al clasificar clientes no interesados como potenciales positivos.\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "- El modelo **Naive Bayes** presenta un rendimiento **decente y balanceado**, destacándose por su **simplicidad computacional** y su **alta capacidad de recuperación de positivos (recall)**.\n",
    "- No obstante, la **muy baja precisión lo hace menos recomendable** en contextos donde los costos de contactar a un cliente no interesado sean altos.\n",
    "- Este modelo puede ser útil como **baseline** o como parte de un **ensamble inicial**, pero es probable que su rendimiento se vea superado por enfoques más sofisticados o no lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847b7c0-c1cc-4d08-9baf-2a00f8e9c686",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-ten\"></a>\n",
    "### 5.10 - Modelo de Deep Learning\n",
    "\n",
    "En esta sección evalúo el rendimiento de un modelo de **Deep Learning** como alternativa al enfoque clásico basado en modelos estadísticos y de aprendizaje automático / machine learning supervisado. Si bien las redes neuronales suelen requerir mayor cantidad de datos y capacidad computacional, creo que pueden resultar útiles para capturar relaciones no lineales complejas entre las variables del dataset.\n",
    "\n",
    "Dado que el problema abordado consiste en una tarea de clasificación binaria, implemento una red neuronal multicapa (**Multilayer Perceptron**, MLP) con una arquitectura simple y entrenada sobre el mismo conjunto de datos procesado previamente. El objetivo es analizar si este enfoque logra mejorar el rendimiento predictivo observado en los modelos previos, como la regresión logística o los árboles de decisión, y si justifica su complejidad adicional en este contexto.\n",
    "\n",
    "Se emplean técnicas de regularización y validación cruzada para evitar el sobreajuste, y se comparan métricas clave como el *F1-score*, la *AUC-ROC*, la *matriz de confusión* y los valores de *precision* y *recall*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58873aa8-191f-4025-a9f2-123207c6a6d9",
   "metadata": {},
   "source": [
    "Primero comienzo por observar la cantidad de variables del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a0b03-e0ef-42af-8cd6-097d93fc2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo la forma/shape del dataset de entrenamiento (la cantidad de filas y especialmente la cantidad de columnas).\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9f16f-ca32-4b44-8fea-55f202a49e5b",
   "metadata": {},
   "source": [
    "Confirmo que la cantidad de variables presentes en el dataset de entrenamiento es igual a 108. Esto es relevante dado que el modelo de deep learning que emplearé necesita especificar las dimensiones del input (es decir, la cantidad de variables a procesar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1374019-cf35-4d55-83f1-8d80503144a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define un modelo secuencial (modelo de red neuronal de tipo \"capa por capa\")\n",
    "model = Sequential()\n",
    "# Uso regularización L2 para evitaro reducir el riesgo de sobreajuste / overfitting (penaliza los pesos grandes)\n",
    "kernel_regularizer_l2 = regularizers.l2(5e-4)\n",
    "# Ahora vendrá la primera capa oculta con 64 neuronas, activación tanh y regularización L2\n",
    "# Se especifica el input_dim igual al número de features de entrada del dataset\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='tanh',kernel_regularizer=kernel_regularizer_l2))\n",
    "# Luego, aquí viene un dropout para reducir sobreajuste: apaga aleatoriamente el 30% de las neuronas\n",
    "model.add(layers.Dropout(0.3))\n",
    "# Segunda capa oculta, también con 64 neuronas y tanh. Se repite la regularización\n",
    "model.add(Dense(64, input_dim=54, activation='tanh',kernel_regularizer=kernel_regularizer_l2))\n",
    "model.add(layers.Dropout(0.3))\n",
    "# Finalizo aquí con una capa de salida con 1 neurona y activación sigmoide para clasificación binaria\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083dbb9b-e1f1-4a37-abfc-1d32eb7631c3",
   "metadata": {},
   "source": [
    "A continuación procedo a compilar el modelo de red neuronal y especifico parámetros específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ccf1bc-321c-4c2a-86bb-78b5606c97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # Función de pérdida adecuada para clasificación binaria\n",
    "    loss='binary_crossentropy',\n",
    "    # Optimizador Adam: combina momentum y adaptatividad en el learning rate\n",
    "    optimizer='adam',\n",
    "    # Métricas a monitorizar durante el entrenamiento y la evaluación\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22294f03-8fd9-4d65-8d58-edff0ef3c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de EarlyStopping como técnica preventiva contra el sobreajuste: \n",
    "# detiene el entrenamiento si la métrica monitoreada no mejora tras un número determinado de épocas\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # Se monitorea la pérdida sobre el conjunto de validación\n",
    "    patience=5, # El entrenamiento se detiene si no mejora después de 5 épocas consecutivas\n",
    "    verbose=1) # Muestra un mensaje cuando se detiene el entrenamiento\n",
    "# Convierto los datos de entrenamiento a arrays NumPy, requeridos por Keras\n",
    "x_train_keras = np.array(X_train)\n",
    "y_train_keras = np.array(y_train)\n",
    "# Ajusto las dimensiones para el vector objetivo (y) a formato (n_samples, 1)\n",
    "y_train_keras = y_train_keras.reshape(y_train_keras.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22914638-e390-458a-9528-b5ec95640135",
   "metadata": {},
   "source": [
    "Ahora paso a entrenar el modelo de Depp Learning luego de su compilación y definición de ajustes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc222f6-8855-4aba-9deb-7bbffbcd2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hago el entrenamiento del modelo con los datos de entrenamiento\n",
    "model.fit(X_train, # Conjunto de entrenamiento con las features\n",
    "          y_train, # Variable objetivo binaria\n",
    "          epochs=200, # Número máximo de épocas de entrenamiento\n",
    "          batch_size=32, # Tamaño del batch: se actualizarán los pesos cada 32 muestras\n",
    "          validation_split=0.2, # Proporción del conjunto de entrenamiento reservado para validación (20%)\n",
    "          verbose=1, # Muestro el progreso del entrenamiento\n",
    "          callbacks=[es_callback]) # Lista de callbacks utilizado (EarlyStopping para evitar sobreajuste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77350deb-58ef-48ea-9bab-9ef03d7d756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión del conjunto de test a arrays de NumPy para compatibilidad con Keras\n",
    "X_test_keras = np.array(X_test)\n",
    "\n",
    "# Me aseguro que las etiquetas de test tengan la forma adecuada (matriz columna)\n",
    "y_test_keras = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "# Evaluación del modelo entrenado sobre el conjunto de test\n",
    "# Devuelve una lista con los valores de las métricas definidas en model.compile()\n",
    "scores = model.evaluate(X_test_keras, y_test_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3bef0-7cad-4dc2-ad8d-671faa4f4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo la métrica de accuracy obtenida en la evaluación del modelo sobre el conjunto de test\n",
    "# 'model.metrics_names[1]' corresponde al nombre de la segunda métrica definida en model.compile(), que es 'accuracy'\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c774c-7c55-4bb1-9ab0-f1d101ff02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test_keras, y_test_keras, verbose=0)\n",
    "print(f'🔹 Loss: {scores[0]:.4f}')\n",
    "print(f'🔹 Accuracy: {scores[1]:.4f}')\n",
    "print(f'🔹 Precision: {scores[2]:.4f}')\n",
    "print(f'🔹 Recall: {scores[3]:.4f}')\n",
    "print(f'🔹 AUC: {scores[4]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5359368-fb5d-4648-a5a3-2db4d794ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizo las predicciones del modelo sobre el conjunto de test\n",
    "y_pred_dl = model.predict(X_test_keras)\n",
    "# Binarizo las probabilidades obtenidas: si la probabilidad es mayor o igual a 0.5 se clasifica como 1, si no como 0\n",
    "y_pred_dl = (y_pred_dl >= 0.5).astype(int)\n",
    "\n",
    "# Calculo el F1 Score comparando las etiquetas reales con las predichas\n",
    "# El F1 Score es útil para evaluar el balance entre precisión y recall, especialmente en datasets desbalanceados\n",
    "f1 = f1_score(y_test_keras, y_pred_dl)\n",
    "# Muestra el valor del F1 Score con 4 decimales\n",
    "print(f'🔹 F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca3df6-3c4a-4bcb-b577-c24537cc547c",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo de Deep Learning\n",
    "\n",
    "A continuación se presentan las observaciones clave sobre el desempeño del modelo de Deep Learning (Red Neuronal) aplicado al conjunto de prueba.\n",
    "\n",
    "### Observaciones clave\n",
    "\n",
    "#### Alta capacidad de discriminación global\n",
    "- El valor **AUC = 0.9315** revela una **excelente capacidad del modelo para distinguir entre clientes que aceptan y no aceptan la oferta**.\n",
    "- La alta AUC, combinada con un buen recall, indica que la red logra **capturar correctamente patrones relevantes**, incluso en presencia de desequilibrio de clases.\n",
    "\n",
    "#### Buen rendimiento en la clase positiva (acepta la oferta)\n",
    "- Para la clase **\"Yes\"** (positiva):\n",
    "  - **Precision: 0.5068** → de cada 100 predicciones positivas, solo ~51 fueron correctas.\n",
    "  - **Recall: 0.7597** → el modelo detecta correctamente un 76% de los clientes que realmente aceptaron la campaña.\n",
    "  - **F1 Score: 0.6080** → representa un **equilibrio razonable entre precisión y recall**, aunque la precisión aún es baja para uso operativo directo.\n",
    "\n",
    "#### Reducción del error global, pero con falsos positivos a considerar\n",
    "- El valor de **Loss: 0.2869** sugiere una **buena minimización del error** en términos de función de pérdida binaria.\n",
    "- Sin embargo, la **precisión moderada** indica una tendencia a generar falsos positivos, lo cual podría traducirse en **contactos comerciales innecesarios** si no se ajusta el umbral de decisión o no se combina con reglas adicionales.\n",
    "\n",
    "#### Buen rendimiento global en métricas generales\n",
    "- **Accuracy: 0.8896** → cerca del 89% de las predicciones fueron correctas.\n",
    "- **Precision global: 0.5068**\n",
    "- **Recall global: 0.7597**\n",
    "- **F1 global: 0.6080**\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "- El modelo de Deep Learning muestra **un rendimiento competitivo en términos de recall y AUC**, siendo capaz de **detectar correctamente una alta proporción de clientes interesados**.\n",
    "- Aunque la **precisión es menor que en otros modelos como XGBoost**, el **modelo es útil para escenarios donde el recall sea prioritario**, por ejemplo, en **etapas tempranas de una campaña** donde se busca maximizar el alcance.\n",
    "- Podría beneficiarse de **ajustes en el umbral de clasificación**, **técnicas de balanceo** adicionales o incluso de una **estrategia de ensamblado** con modelos más precisos para mejorar la eficiencia final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e189a1-753d-46b5-b6da-a1eac8018fa3",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-eleven\"></a>\n",
    "### 5.11 - Comparación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1b93f-1879-46c4-8872-3147ff555aee",
   "metadata": {},
   "source": [
    "Se realizaron comparaciones previas entre modelos, pero en esta sección profundizo la comparación para obtener una conclusión definitiva entre los renidmientos de cada modelo frente a la tarea de clasificación que necesitamos para este proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea920dc3-7221-4a4d-b10f-e18604ae5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de clasificadores con parámetros para tratar el desbalance de clases donde sea posible\n",
    "clf_dict = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=3000, random_state=8),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=8),\n",
    "    'Random Forest': RandomForestClassifier(criterion='entropy', class_weight='balanced', random_state=8),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=7.85, use_label_encoder=False, eval_metric='logloss', seed=8),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),  # KNN no soporta class_weight\n",
    "    'Naive Bayes': GaussianNB(priors=[0.113, 0.887])  # Ajuste manual de probabilidades según proporción real\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d34d2e-0b9d-4d90-8869-cac8c8ec7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre_modelo, modelo in clf_dict.items():\n",
    "    print(f\"\\n{'='*40}\\n🔎 Modelo: {nombre_modelo}\\n{'='*40}\")\n",
    "    \n",
    "    # Entrenamiento\n",
    "    modelo.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicción de clases\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    \n",
    "    # Predicción de probabilidades\n",
    "    if hasattr(modelo, \"predict_proba\"):\n",
    "        y_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba = None  \n",
    "    \n",
    "    # Evaluación\n",
    "    saca_metricas(y_test, y_pred, y_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d388b6b-5c5b-4466-a025-88b438b15609",
   "metadata": {},
   "source": [
    "<a id=\"section-five-subsection-twelve\"></a>\n",
    "### 5.12 - Tuneo de hiperparámetros para el modelo seleccionado\n",
    "\n",
    "Una vez identificado XGBoost como el modelo con mejor desempeño general en la etapa comparativa, se procedió a optimizar sus hiperparámetros con el objetivo de mejorar su capacidad predictiva y adaptar el modelo de manera más precisa a las características del conjunto de datos.\n",
    "\n",
    "La optimización de hiperparámetros se llevó a cabo mediante **GridSearchCV**, una técnica de búsqueda exhaustiva que permite evaluar todas las combinaciones posibles dentro de un espacio definido de hiperparámetros. Este enfoque permite encontrar la configuración óptima que maximice el rendimiento del modelo según una métrica de evaluación determinada (en este caso, `roc_auc`).\n",
    "\n",
    "Los hiperparámetros evaluados incluyen:\n",
    "\n",
    "- `n_estimators`: número de árboles a construir.\n",
    "- `max_depth`: profundidad máxima de cada árbol.\n",
    "- `learning_rate`: tasa de aprendizaje utilizada para la actualización de pesos.\n",
    "- `subsample`: proporción de muestras utilizadas en cada iteración.\n",
    "- `colsample_bytree`: proporción de columnas utilizadas por cada árbol.\n",
    "\n",
    "El proceso se llevó a cabo utilizando validación cruzada de 5 pliegues, lo que garantiza una evaluación robusta del modelo y ayuda a mitigar problemas de sobreajuste. A continuación, se presentan los resultados obtenidos y la mejor combinación de hiperparámetros seleccionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e58ac-9c09-4e00-99a8-8d3c09d1de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se instancia el modelo base de XGBoost con una métrica de evaluación adecuada\n",
    "# y una semilla fija para asegurar reproducibilidad.\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Se define la grilla de hiperparámetros a evaluar con GridSearchCV.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200], # Número total de árboles a entrenar                  \n",
    "    'max_depth': [3, 5], # Profundidad máxima de cada árbol, controla la complejidad del modelo.                         \n",
    "    'learning_rate': [0.05, 0.1], # Tasa de aprendizaje que reduce la contribución de cada árbol.               \n",
    "    'subsample': [0.85, 1.0], # Proporción de muestras utilizadas en cada iteración (para evitar sobreajuste).                   \n",
    "    'colsample_bytree': [0.85, 1.0], # Proporción de columnas (features) muestreadas por árbol.            \n",
    "    'scale_pos_weight': [1, 5, 7.85] # Peso relativo de la clase positiva, útil para desbalance de clases.            \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87350945-1aea-4a65-8823-2878d213147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se configura GridSearchCV para realizar búsqueda exhaustiva de combinaciones de hiperparámetros.\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb, # Modelo base (XGBoost) a tunear.\n",
    "    param_grid=param_grid, # Grilla de hiperparámetros definida previamente.\n",
    "    scoring='f1', # Métrica objetivo para seleccionar el mejor modelo (F1 Score).\n",
    "    cv=3, # Número de particiones para validación cruzada (3-fold CV).                      \n",
    "    n_jobs=-1, # Usa todos los núcleos disponibles para acelerar la búsqueda.\n",
    "    verbose=2 # Nivel de detalle en la salida durante el entrenamiento.\n",
    ")\n",
    "\n",
    "# Se ajusta el modelo a los datos de entrenamiento utilizando validación cruzada.\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96acf1a-be90-47c1-983f-b300216bda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprimo los mejores hiperparámetros encontrados para el modelo XGBoost.\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80b02f-284f-4b66-9eac-a4a1fc690118",
   "metadata": {},
   "source": [
    "### Análisis de los Hiperparámetros Seleccionados\n",
    "\n",
    "Los hiperparámetros seleccionados durante la búsqueda con `GridSearchCV` han contribuido de manera significativa al rendimiento del modelo:\n",
    "\n",
    "- **`max_depth` moderado**: Un valor adecuado de profundidad controla el sobreajuste permitiendo al modelo capturar relaciones complejas sin memorizar ruido. Esto es clave dado que los datos poseen múltiples variables categóricas y relaciones no lineales.\n",
    "\n",
    "- **`learning_rate` bajo**: Una tasa de aprendizaje más baja (por ejemplo, 0.05 o 0.1) hace que el modelo aprenda de forma más gradual y estable, lo cual tiende a mejorar el rendimiento general en validación cruzada.\n",
    "\n",
    "- **`n_estimators` elevado**: Usar un número mayor de árboles junto a un `learning_rate` bajo permite que el modelo construya una solución más robusta y generalizable, incrementando el AUC y la capacidad de generalización.\n",
    "\n",
    "- **`subsample` y `colsample_bytree` < 1**: Estas tasas de muestreo controlan el sobreajuste al limitar el número de observaciones y características usadas por cada árbol. Son especialmente útiles en datasets como este, donde hay riesgo de que el modelo se adapte demasiado a los patrones particulares del conjunto de entrenamiento.\n",
    "\n",
    "- **`scale_pos_weight` ajustado**: En contextos de clases desbalanceadas, este parámetro permite al modelo penalizar más los errores de clasificación en la clase minoritaria (“Yes”), lo cual explica el fuerte aumento en el recall sin sacrificar demasiado la precisión.\n",
    "\n",
    "En conjunto, estos valores permiten que `XGBoost` actúe como un **modelo regulado, eficiente y con buena capacidad de generalización**, maximizando su utilidad práctica en campañas reales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ec173-727e-4020-b976-16d51d26db7a",
   "metadata": {},
   "source": [
    "Estos valores obtenidos fueron luego utilizados para reentrenar el modelo XGBoost final con el objetivo de maximizar su desempeño predictivo sobre datos no vistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b55932-a6b5-4d9d-a7be-ecef554c558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se extrae el mejor modelo ajustado (con los mejores hiperparámetros) del objeto GridSearchCV\n",
    "mejor_modelo_xgb = grid_search.best_estimator_\n",
    "# Se generan las predicciones de clase sobre el conjunto de test\n",
    "y_pred = mejor_modelo_xgb.predict(X_test)\n",
    "# Se generan las probabilidades de predicción para la clase positiva (label = 1)\n",
    "y_proba = mejor_modelo_xgb.predict_proba(X_test)[:, 1]\n",
    "# Se evalúa el desempeño del modelo utilizando las métricas definidas en la función personalizada 'saca_metricas'\n",
    "saca_metricas(y_test, y_pred, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5884fa-c30d-418b-aa9a-b62cd23bcd19",
   "metadata": {},
   "source": [
    "### Evaluación del Modelo XGBoost tras el Tuneo de Hiperparámetros\n",
    "\n",
    "Los resultados del modelo optimizado mediante `GridSearchCV` muestran una mejora significativa en la capacidad del clasificador para identificar correctamente los casos positivos (clientes que aceptan la oferta de marketing):\n",
    "\n",
    "🔹 **AUC = 0.95**: El área bajo la curva ROC indica un excelente desempeño general del modelo, muy por encima del azar (0.5). El modelo discrimina bien entre las clases.\n",
    "\n",
    "🔹 **Recall = 0.81** para la clase `\"Yes\"`: Este valor refleja una gran capacidad para captar los casos positivos, lo cual es crucial en este contexto si se busca maximizar la tasa de respuesta de la campaña.\n",
    "\n",
    "🔹 **Precision = 0.54**: Si bien el recall es alto, la precisión es más baja, lo que indica una cantidad significativa de falsos positivos. Esto es aceptable en campañas donde el costo de contactar a un cliente que no convertirá es bajo comparado con el beneficio de captar uno que sí lo hará.\n",
    "\n",
    "🔹 **F1 Score = 0.65**: El balance entre precisión y recall es razonable, confirmando que el modelo logra un buen compromiso entre ambas métricas.\n",
    "\n",
    "🔹 **Accuracy = 0.90**: Aunque elevado, debe interpretarse con precaución dada la desbalanceada distribución de clases (mayoría de “No”).\n",
    "\n",
    "🔹 **Matriz de Confusión:**\n",
    "\n",
    "Verdaderos negativos: 6664\n",
    "Falsos positivos:     644\n",
    "Verdaderos positivos: 749\n",
    "Falsos negativos:     179\n",
    "\n",
    "\n",
    "Esto indica que el modelo logra identificar correctamente el 81% de los clientes que aceptan la oferta, aunque todavía existen errores de clasificación (clientes que no aceptarán pero son identificados como positivos).\n",
    "\n",
    "En conjunto, estas métricas y el gráfico ROC sugieren que el modelo XGBoost optimizado representa una **solución robusta para predecir la aceptación de campañas**, especialmente si se prioriza la captación de clientes potenciales por encima del costo de algunos falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff3c08-4b4c-42a3-abc6-aab70381c0c4",
   "metadata": {},
   "source": [
    "Ahora, muestro las 10 mejores combinaciones de hiperparámetros junto con su score promedio y desviación estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6cb539-8f1e-45e8-a2cb-c9637116f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos los resultados del GridSearchCV a un DataFrame para facilitar el análisis\n",
    "resultados = pd.DataFrame(grid_search.cv_results_)\n",
    "# Ordenamos las filas del DataFrame según el F1 Score promedio obtenido en validación cruzada (de mayor a menor)\n",
    "resultados = resultados.sort_values(by=\"mean_test_score\", ascending=False)\n",
    "# Mostramos las 10 mejores combinaciones de hiperparámetros junto con su score promedio y desviación estándar\n",
    "resultados[[\"params\", \"mean_test_score\", \"std_test_score\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b6bb3-98d9-4025-b393-ed5925865e0f",
   "metadata": {},
   "source": [
    "Una vez obtenidos los mejores hiperparámetros entonces procedo a guardarlo como el **modelo final**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc838126-ca81-4a02-b3d7-a80f0d47ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos el mejor modelo encontrado por GridSearchCV (ya entrenado) a una variable para usarlo como modelo final\n",
    "model_final = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f12eed-e719-484d-857d-fba46109abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo el tipo de objeto del modelo final; en este caso, un clasificador XGBoost ya entrenado\n",
    "print(type(model_final))  \n",
    "# Muestro todos los hiperparámetros (tanto los definidos manualmente como los optimizados por GridSearchCV)\n",
    "print(model_final.get_params())  \n",
    "# Realizo predicciones sobre el conjunto de test con el modelo final optimizado\n",
    "model_final.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee350a7-3c86-4925-8fd3-3070f62f32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar a disco el modelo final ajustado\n",
    "joblib.dump(model_final, 'modelo_final_xgb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71469b5f-2f42-4362-b560-df011f7ac5f6",
   "metadata": {},
   "source": [
    "Este modelo puede ahora ser evaluado, exportado o **integrado dentro de un pipeline de despliegue para su uso en aplicaciones reales**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d2cb8-af9a-41ca-bc5f-b4d6e758ba0a",
   "metadata": {},
   "source": [
    "<a id=\"section-six\"></a>\n",
    "<h2><strong>6- INTERPRETABILIDAD Y EXPLICABILIDAD DE MODELOS</strong> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cfa9a-675e-4a4c-9dfc-9e62d5a4a851",
   "metadata": {},
   "source": [
    "En esta sección se aborda la importancia de comprender el funcionamiento interno del modelo predictivo desarrollado, con el fin de generar confianza, validar su comportamiento y facilitar la toma de decisiones basadas en sus resultados. La **interpretabilidad y explicabilidad** son aspectos clave para garantizar que las predicciones no solo sean precisas, sino también comprensibles para usuarios técnicos y no técnicos.\n",
    "\n",
    "Se explorarán técnicas y herramientas que permiten analizar la contribución de las variables de entrada, identificar patrones relevantes y explicar las decisiones del modelo, especialmente en contextos donde la transparencia es fundamental para la adopción de la solución (como es el sector bancario que nos compete aquí en este proyecto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832291b3-9cf2-4839-8d0d-a647508b1c34",
   "metadata": {},
   "source": [
    "<a id=\"section-six-subsection-one\"></a>\n",
    "### **6.1 Feature importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a15a7d-3d63-464a-aef2-8e08853ac175",
   "metadata": {},
   "source": [
    "El **análisis de importancia de variables** permite identificar cuáles características del conjunto de datos tienen mayor influencia en las predicciones del modelo. Esta información es fundamental para comprender el comportamiento del modelo, detectar posibles sesgos y validar si las variables más relevantes tienen sentido desde el punto de vista del dominio del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290ca79-4c91-49c7-9da3-cdb0710a53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las importancias de las variables del modelo final\n",
    "feat_importances = pd.DataFrame(model_final.feature_importances_, index=X_train.columns, columns=[\"Importance\"])\n",
    "# Ordenar las variables de mayor a menor importancia\n",
    "feat_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Seleccionar las 25 variables más importantes\n",
    "top_features = feat_importances.head(25)\n",
    "\n",
    "# Grafico las 25 variables más relevantes para el modelo final\n",
    "top_features.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Top 25 Features más importantes\")\n",
    "plt.ylabel(\"Importancia\")\n",
    "plt.xticks(rotation=45, ha='right') # Roto las etiquetas del eje X para mejorar la legibilidaddel gráfico\n",
    "plt.tight_layout() # Ajusto el layout para evitar recortes en el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ee0c7-bd81-4b7c-ad02-34954e24c54c",
   "metadata": {},
   "source": [
    "#### Insights preliminares de las features\n",
    "\n",
    "1. **La variable `duration` domina ampliamente la predicción del modelo**\n",
    "   - Es, por mucho, la característica más influyente.\n",
    "   - Esto es esperable, ya que la duración de la llamada está altamente correlacionada con la respuesta positiva del cliente: a mayor duración, mayor probabilidad de aceptación.\n",
    "\n",
    "2. **Variables económicas externas como `euribor3m`, `cons.conf.idx` y `cons.price.idx` tienen alta importancia**\n",
    "   - Estas variables macroeconómicas afectan significativamente la decisión del cliente, reflejando que el contexto económico influye en la predisposición a contratar productos financieros.\n",
    "   - Es recomendable tener en cuenta el entorno macroeconómico al lanzar campañas.\n",
    "\n",
    "3. **Variables como `default_no`, `emp.var.rate` y `day_of_week` también presentan relevancia destacada**\n",
    "   - `default_no` sugiere que clientes con historial crediticio limpio tienen más probabilidad de aceptar.\n",
    "   - `day_of_week` indica que el día del contacto podría tener un efecto relevante en la tasa de conversión.\n",
    "\n",
    "4. **El tipo de contacto (`contact_cellular`) y la frecuencia de contactos previos (`campaign`, `pdays`, `previous`) son relevantes**\n",
    "   - Reflejan el impacto del canal y la intensidad de la campaña sobre la respuesta del cliente.\n",
    "\n",
    "5. **Variables categóricas derivadas como `job_grouped`, `education_grouped`, `life_stage` y `socio-economic` aparecen con menor importancia**\n",
    "   - Aunque su impacto individual es bajo, aportan valor contextual y pueden ser relevantes en combinación con otras variables.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusiones parciales de las features\n",
    "\n",
    "- El modelo se apoya fuertemente en variables que describen la **interacción durante la campaña** (especialmente `duration`) y en **indicadores macroeconómicos**.\n",
    "- El perfil sociodemográfico del cliente tiene un impacto moderado, lo cual sugiere que la disposición a contratar el producto depende más del contexto que del perfil estático del cliente.\n",
    "- Estos hallazgos permiten identificar oportunidades de optimización para campañas futuras:\n",
    "  - Elegir los días más efectivos para contactar (`day_of_week`).\n",
    "  - Evitar campañas demasiado insistentes (controlar `campaign` y `pdays`).\n",
    "  - Lanzar campañas en momentos macroeconómicamente favorables (`euribor3m`, `cons.conf.idx`).\n",
    "\n",
    "Este análisis refuerza la importancia de combinar información contextual, del cliente y del entorno económico para mejorar la eficacia de las campañas de marketing predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebec13-f38b-4dde-bc17-5307bafcc24f",
   "metadata": {},
   "source": [
    "<a id=\"section-six-subsection-two\"></a>\n",
    "### **6.2 SHAP y LIME para interpretación local/global**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba84fb-bf3e-45ab-b6ec-d591f7e87383",
   "metadata": {},
   "source": [
    "### Explicabilidad Local con LIME\n",
    "\n",
    "Con el objetivo de comprender el comportamiento del modelo a nivel individual, se utilizó la técnica **LIME** (*Local Interpretable Model-agnostic Explanations*). Esta herramienta permite generar explicaciones locales e interpretables para modelos complejos, proporcionando información sobre cómo cada característica influyó en la predicción de una instancia específica.\n",
    "\n",
    "LIME actúa generando ligeras perturbaciones alrededor del punto de interés y entrenando un modelo interpretable (por lo general, lineal) sobre estos datos simulados. De este modo, aproxima el comportamiento del modelo original en un entorno local, permitiendo identificar las variables que más contribuyeron a la predicción, así como el sentido de dicha contribución (positivo o negativo).\n",
    "\n",
    "En este caso, se analizó una observación particular del conjunto de prueba, y se generó una explicación visual que destaca las 30 características más influyentes en la decisión del modelo. Esta aproximación permite validar las decisiones del modelo y facilita su interpretación para usuarios finales o stakeholders no técnicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc512c92-069d-44e7-9bb4-3ac52aff0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea una instancia de LimeTabularExplainer para generar explicaciones locales del modelo.\n",
    "# Esta clase entrena un modelo interpretable (e.g., lineal) sobre datos perturbados de una instancia puntual.\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, # Datos de entrenamiento en formato numpy array\n",
    "                                                   mode='classification', # Tipo de tarea: clasificación\n",
    "                                                   training_labels=y_train, # Etiquetas reales del conjunto de entrenamiento\n",
    "                                                   feature_names=X_train.columns, # Nombres de las variables para que la explicación sea legible\n",
    "                                                   random_state=42) # Fijación de semilla para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6b783-bf2c-477c-997c-ee887e488553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciono un registro del dataset de pruba al azar junto con sus valores.\n",
    "X_test.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d675a-d36e-4317-9a83-f0be33914d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtienen las probabilidades predichas por el modelo.\n",
    "model_final.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031bfe5-a382-483f-8728-c5223bc5633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtienen las predicciones finales del modelo para el conjunto de test. Cada valor corresponde a la clase predicha (0 o 1) para cada observación.\n",
    "model_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0567b7e-86de-4468-8ad6-7da3464587e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero aquí una explicación local con LIME para una instancia específica del conjunto de test.\n",
    "# En este caso, analizo la observación en la posición 100.\n",
    "# LIME utiliza el método predict_proba del modelo para estimar la contribución de cada variable.\n",
    "# El parámetro num_features=30 indica que se mostrarán las 30 variables más influyentes en la predicción en esta celda.\n",
    "exp=explainer.explain_instance(X_test.iloc[100], model_final.predict_proba,num_features=30)\n",
    "\n",
    "# Luego muestro la explicación generada de forma interactiva dentro del notebook.\n",
    "# El gráfico resultante indica el impacto (positivo o negativo, con color naranja o azul respectivamente) por variable sobre la predicción del modelo.\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02fe5a-5e9c-4b62-9ef0-0271282e05c1",
   "metadata": {},
   "source": [
    "En la celda anteriores limité la cantidad de variables a considerar por LIME, en cambio en la siguiente celda incluyo todas las variables para dar una explicabilidad más exhaustiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6876b6e-368f-4a1f-bf12-ab84e14afdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero aquí nuevamente la explicación local con LIME para una instancia específica del conjunto de test (registro 100).\n",
    "# Ahora habilito que aparezcan todas las variables y su explicabilidad.\n",
    "exp.show_in_notebook(show_all=True)\n",
    "\n",
    "# Se guarda la explicación generada en un archivo HTML interactivo para su consulta o presentación posterior.\n",
    "exp.save_to_file('explanation.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89e12f-5003-4752-b28a-e118f0c1efa7",
   "metadata": {},
   "source": [
    "### Explicabilidad Local con LIME\n",
    "\n",
    "Se utilizó la herramienta **LIME (Local Interpretable Model-Agnostic Explanations)** para analizar el comportamiento del modelo XGBoost sobre un caso específico del conjunto de testeo. LIME permite comprender **por qué el modelo tomó una decisión concreta**, revelando qué características influyeron más en la predicción de esa instancia.\n",
    "\n",
    "#### Resultado de la Predicción\n",
    "\n",
    "- **Probabilidad predicha de aceptación (clase “Yes”)**: **0.72**\n",
    "- **Probabilidad de no aceptación**: 0.28  \n",
    "➡️ El modelo predice con alta confianza que el cliente aceptará la oferta.\n",
    "\n",
    "#### Principales características que contribuyeron positivamente (hacia clase “Yes”):\n",
    "\n",
    "- `age = 0.42`: la edad del cliente (escalada entre 0 y 1) se vincula positivamente con la predicción.\n",
    "- `month = 0.33`: el mes en que se realizó el contacto parece tener influencia positiva.\n",
    "- `day_of_week = 1.00`: día de la semana puede haber coincidido con una tendencia de mayor conversión.\n",
    "- `education_university.degree = 1.00`: el cliente posee título universitario, lo que se relaciona positivamente con la aceptación.\n",
    "- `marital_single = 1.00`: clientes solteros parecen más propensos a aceptar la campaña.\n",
    "- `housing_yes = 1.00` y `loan_yes = 1.00`: aunque se tiene préstamo y vivienda, en este caso no actuaron como frenos para la predicción positiva.\n",
    "- `contact_cellular = 1.00`: contacto por celular tiende a estar asociado a respuestas más afirmativas.\n",
    "- `life_stage_middle aged & single = 1.00` y `job_grouped_White-collar = 1.00`: estos perfiles socio-demográficos están asociados a una mayor propensión a aceptar.\n",
    "\n",
    "#### Características con impacto neutro o bajo:\n",
    "\n",
    "- Variables como `pdays`, `previous`, `campaign`, `job_blue-collar`, `poutcome_failure`, y muchas combinaciones socioeconómicas no presentaron impacto relevante (valor = 0.00), lo que indica que no influyeron en esta predicción particular.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusiones de la explicación local\n",
    "\n",
    "- La predicción **no solo se apoya en variables cuantitativas**, como `age`, `duration`, o `emp.var.rate`, sino también en **variables categóricas codificadas**, relacionadas con nivel educativo, tipo de empleo y estado civil.\n",
    "- LIME revela que el modelo **asocia ciertos perfiles sociodemográficos con mayor probabilidad de aceptación**, en línea con el análisis general realizado con SHAP.\n",
    "- Esta explicación local valida que el modelo no es una \"caja negra\", y que su decisión se basa en **factores interpretables y coherentes con el dominio del negocio**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93312fd0-209d-449e-802e-22dc9e5c1f7d",
   "metadata": {},
   "source": [
    "### Explicabilidad Global con SHAP\n",
    "\n",
    "Para entender el comportamiento general del modelo y la importancia relativa de cada variable, se utilizó la técnica **SHAP** (*SHapley Additive exPlanations*). Esta metodología calcula, para cada predicción individual, la contribución de cada característica utilizando conceptos de la teoría de juegos, y luego permite agregar esas explicaciones para obtener una visión global.\n",
    "\n",
    "En este trabajo, se calcularon los valores SHAP para una muestra representativa del conjunto de test (100 observaciones). El gráfico resumen (`summary_plot`) generado a partir de estos valores visualiza la influencia promedio y el efecto que cada variable tiene sobre las predicciones del modelo.\n",
    "\n",
    "Esta aproximación ofrece una explicación global del modelo, resaltando qué variables son más determinantes y cómo sus diferentes valores afectan la salida del clasificador, facilitando la interpretación y validación del modelo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be666d-5128-45ce-b629-a61ba432c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un objeto explainer SHAP para el modelo entrenado.\n",
    "explainer = shap.Explainer(model_final)\n",
    "# Se calculan los valores SHAP para las primeras 100 instancias del conjunto de test,obteniendo la contribución de cada variable para cada predicción.\n",
    "shap_values = explainer.shap_values(X_test.iloc[0:100])\n",
    "# Se genera un gráfico resumen que muestra la importancia global de las variables y cómo sus valores afectan las predicciones a través de la distribución de valores SHAP.\n",
    "shap.summary_plot(shap_values, X_test.iloc[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac7f94-a1d0-43fb-b2f6-bd167e6f3f33",
   "metadata": {},
   "source": [
    "## Interpretabilidad Global del Modelo – SHAP Summary Plot\n",
    "\n",
    "En el gráfico SHAP summary se observa lo siguiente:\n",
    "\n",
    "- **`duration`** es, con diferencia, la variable que más impacto tiene en la predicción del modelo. A mayor duración de la llamada (color rosa), mayor es la probabilidad de que el cliente acepte el producto. Esto se alinea con la lógica comercial: llamadas más largas suelen indicar mayor interés.\n",
    "- Variables económicas como **`emp.var.rate`** (tasa de variación de empleo) y **`euribor3m`** (tipo de interés a 3 meses) también tienen fuerte influencia. Sus valores más bajos (color azul) están asociados a un aumento en la probabilidad de aceptación, lo que puede reflejar épocas de menor confianza económica donde las campañas bancarias son más efectivas.\n",
    "- Variables de comportamiento como **`campaign`** (número de contactos durante la campaña), **`pdays`** y **`previous`** muestran que más contactos previos tienden a tener un efecto negativo si no fueron exitosos, reflejando potencial saturación del cliente.\n",
    "- Variables categóricas transformadas, como **`contact_cellular`**, **`default_no`**, **`education_university.degree`**, entre otras, también muestran influencia aunque menor, evidenciando el impacto de factores demográficos y de canal de contacto.\n",
    "\n",
    "Este gráfico proporciona una visión clara y transparente de cómo las diferentes variables interactúan con el modelo, reforzando la confianza en su funcionamiento y permitiendo una mejor toma de decisiones en un entorno bancario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef585c4-cbd1-4431-8d2f-cabef3ae84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular los valores SHAP\n",
    "explainer = shap.Explainer(model_final)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Lo puedo guardar el gráfico  como un png para presentaciones posteriores.\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.savefig(\"shap_summary_plot.png\", bbox_inches='tight', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf1a54-f984-4d80-b32a-bac2c7b6e250",
   "metadata": {},
   "source": [
    "<a id=\"section-six-subsection-three\"></a>\n",
    "### **6.3 Discusión sobre sesgos y robustez del modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50acc60b-c027-46cb-93ec-18b6a49a79ae",
   "metadata": {},
   "source": [
    "### Discusión sobre Sesgos y Robustez del Modelo\n",
    "\n",
    "El análisis predictivo realizado se basó en un dataset con características demográficas, laborales y de interacción previa con la entidad bancaria durante campañas de marketing. Si bien el modelo XGBoost optimizado alcanzó métricas destacadas (AUC = 0.95, F1 Score = 0.65, Recall = 0.81), es fundamental evaluar críticamente su **robustez ante variaciones de los datos** y la posible existencia de **sesgos inherentes**.\n",
    "\n",
    "#### Posibles sesgos del modelo\n",
    "\n",
    "- **Desbalance de clases**: La variable objetivo presenta un marcado desbalance, con una mayoría de clientes que no aceptaron la oferta. Esto puede inducir al modelo a favorecer la clase negativa (\"No\"), afectando la precisión sobre la clase minoritaria. Se aplicaron técnicas y métricas adecuadas como F1 y AUC para mitigar este efecto, aunque aún persisten falsos positivos y negativos.\n",
    "\n",
    "- **Variables socioeconómicas sensibles**: Algunas variables utilizadas, como `education`, `job`, `marital status` o combinaciones socioeconómicas, pueden estar correlacionadas con factores sensibles o discriminatorios. Si bien se incluyeron por su valor predictivo, se recomienda monitorear su influencia para evitar posibles sesgos indirectos en la toma de decisiones.\n",
    "\n",
    "- **Información histórica y sesgada por campañas previas**: Variables como `pdays` o `previous` reflejan interacciones anteriores con el cliente, lo que podría introducir un sesgo por selección si las campañas anteriores no fueron aleatorias.\n",
    "\n",
    "#### Robustez del modelo\n",
    "\n",
    "- **Validación cruzada estratificada**: Se implementó validación cruzada estratificada para asegurar estabilidad en los resultados, logrando un F1 promedio consistente durante el entrenamiento.\n",
    "\n",
    "- **Rendimiento estable ante nuevos datos**: El desempeño sobre el conjunto de test refleja una generalización sólida del modelo, sin indicios de sobreajuste. Las métricas mantienen un nivel elevado y balanceado, especialmente el AUC, que demuestra una buena capacidad de discriminación global.\n",
    "\n",
    "- **Explicabilidad con SHAP y LIME**: Se incorporaron herramientas de interpretabilidad (SHAP y LIME) que permiten identificar las variables más influyentes en las predicciones. Estas explicaciones refuerzan la confianza en el modelo al mostrar un razonamiento coherente con el dominio del problema.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "El modelo XGBoost muestra una buena robustez general y capacidad de generalización. Sin embargo, se identifican potenciales fuentes de sesgo que deben considerarse si se busca aplicar el modelo en producción. Es recomendable complementar el modelo con evaluaciones éticas y análisis de equidad, especialmente si se utiliza en contextos sensibles o se automatizan decisiones comerciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b9d36c-b8d5-4454-8085-dade3d1e632c",
   "metadata": {},
   "source": [
    "<a id=\"section-seven\"></a>\n",
    "<h2><strong>7- EVALUACIÓN FINAL Y SELECCIÓN DE MODELO FINAL</strong> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb2389-35cd-4482-9261-bf0450bcfaa1",
   "metadata": {},
   "source": [
    "En esta sección se presenta la **evaluación definitiva de los modelos desarrollados** utilizando métricas adecuadas para el problema planteado. Se comparan los resultados obtenidos para determinar cuál fue el modelo que ofrece el mejor desempeño y equilibrio entre precisión, robustez y capacidad de generalización.\n",
    "\n",
    "Además, se describen los criterios considerados para la **selección del modelo final**, teniendo en cuenta tanto aspectos cuantitativos como cualitativos, con el objetivo de elegir la solución más adecuada para su aplicación práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07541845-a65b-4158-85fd-b8db66879e33",
   "metadata": {},
   "source": [
    "<a id=\"section-seven-subsection-one\"></a>\n",
    "### **7.1 Comparación global de métricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1dded4-2f59-43b2-8d26-fbbdb50b6276",
   "metadata": {},
   "source": [
    "A continuación, se presenta el gráfico comparativo final entre los modelos utilizados y sus valores obtenidos para las diversas métricas de evaluación empleadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf01af-34f0-40a2-a5f0-6e1c000f3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de las métricas obtenidas para cada modelo.\n",
    "data = {\n",
    "    'Modelo': [\n",
    "        'XGBoost', 'Random Forest', 'Logistic Regression',\n",
    "        'Decision Tree', 'K-Nearest Neighbors', 'Naive Bayes', 'Deep Learning'\n",
    "    ],\n",
    "    'AUC': [0.945, 0.941, 0.933, 0.750, 0.736, 0.776, 0.9315],\n",
    "    'F1 Score': [0.6241, 0.5794, 0.5894, 0.5370, 0.3871, 0.3499, 0.6080],\n",
    "    'Precision': [0.4834, 0.6049, 0.4498, 0.5067, 0.2972, 0.2277, 0.5068],\n",
    "    'Recall': [0.8804, 0.5560, 0.8545, 0.5711, 0.5550, 0.7554, 0.7597],\n",
    "    'Accuracy': [0.8805, 0.9091, 0.8658, 0.8890, 0.8020, 0.6837, 0.8896]\n",
    "}\n",
    "\n",
    "# Creo el DataFrame y lo transformar a formato largo (long-form)\n",
    "df = pd.DataFrame(data)\n",
    "df_melted = df.melt(id_vars='Modelo', var_name='Métrica', value_name='Valor')\n",
    "\n",
    "# Ajusto el estilo y visualización\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Gráfico de barras\n",
    "ax = sns.barplot(data=df_melted, x='Métrica', y='Valor', hue='Modelo', palette='Set2')\n",
    "\n",
    "# Ajustes estéticos\n",
    "plt.title('Comparación de Métricas por Modelo', fontsize=16)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Valor', fontsize=12)\n",
    "plt.xlabel('Métrica', fontsize=12)\n",
    "plt.legend(title='Modelo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Muestro el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63d454-5af4-40fc-a183-2b8d3a1923ab",
   "metadata": {},
   "source": [
    "## Comparación de Modelos y Conclusión General\n",
    "\n",
    "Se evaluaron seis modelos de clasificación sobre el conjunto de test, ajustando los parámetros para abordar el desbalance de clases donde fue posible. A continuación, se resumen las observaciones más relevantes y se comparan los modelos en función de métricas clave como **AUC**, **F1 Score**, **Precisión**, **Recall** y **Accuracy**.\n",
    "\n",
    "### Comparación General de Desempeño\n",
    "\n",
    "| Modelo               | AUC    | F1 Score | Precision | Recall | Accuracy |\n",
    "|----------------------|--------|----------|-----------|--------|----------|\n",
    "| **XGBoost**          | 0.945  | **0.6241** | 0.4834    | **0.8804** | 0.8805   |\n",
    "| **Random Forest**    | 0.941  | 0.5794   | **0.6049**| 0.5560 | **0.9091** |\n",
    "| **Logistic Regression** | 0.933 | 0.5894  | 0.4498    | 0.8545 | 0.8658   |\n",
    "| **Decision Tree**    | 0.750  | 0.5370   | 0.5067    | 0.5711 | 0.8890   |\n",
    "| **K-Nearest Neighbors** | 0.736 | 0.3871 | 0.2972    | 0.5550 | 0.8020   |\n",
    "| **Naive Bayes**      | 0.776  | 0.3499   | 0.2277    | 0.7554 | 0.6837   |\n",
    "\n",
    "### Observaciones Clave\n",
    "\n",
    "#### **Mejor rendimiento global: XGBoost**\n",
    "- Logra el **mejor equilibrio entre recall y precisión para la clase positiva**.\n",
    "- **F1 Score más alto (0.6241)**, reflejando una combinación efectiva de precisión y cobertura.\n",
    "- **Recall sobresaliente (0.88)**: identifica correctamente casi el 90% de los clientes que aceptan la campaña.\n",
    "- Excelente **AUC (0.945)**, lo que confirma una fuerte capacidad de discriminación.\n",
    "\n",
    "#### **Random Forest**: Precisión elevada, pero menor cobertura\n",
    "- Aunque su **precision (0.60)** supera a XGBoost, su **recall (0.56)** es considerablemente menor, por lo que **pierde muchas oportunidades de identificar clientes interesados**.\n",
    "- Su **F1 score (0.5794)** y **accuracy general (0.91)** lo convierten en una opción muy sólida si se prioriza **minimizar falsos positivos**.\n",
    "\n",
    "#### **Modelos base (KNN, Naive Bayes)** con desempeño débil\n",
    "- Ambos muestran **precisión muy baja** y **fuerte sesgo hacia la clase mayoritaria**, lo que los hace poco adecuados en este contexto.\n",
    "- El Naive Bayes obtiene una F1 de apenas **0.3499**, y KNN **0.3871**, con una AUC muy inferior a los mejores modelos.\n",
    "\n",
    "#### **Regresión Logística**: buen recall, pero precisión limitada\n",
    "- Presenta un **recall competitivo (0.8545)**, cercano al de XGBoost, pero su **precisión (0.45)** es inferior.\n",
    "- Puede ser útil como benchmark, pero **genera una mayor cantidad de falsos positivos** (970), lo cual implica costos adicionales para el área de negocio.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión Final\n",
    "\n",
    "- **XGBoost emerge como el modelo más equilibrado y eficaz**, especialmente si se busca **maximizar la identificación de clientes interesados sin comprometer demasiado la precisión**.\n",
    "- **Random Forest** es también una opción robusta si el objetivo es **priorizar la precisión y reducir falsos positivos**, aunque a costa de perder parte de los clientes potenciales.\n",
    "- **Modelos simples como Naive Bayes y KNN no son competitivos** para esta tarea, incluso con ajustes al desbalance de clases.\n",
    "\n",
    "> En función del objetivo del negocio, la elección del modelo ideal dependerá de si se desea **maximizar cobertura (recall)** o **precisión operacional**. No obstante, XGBoost ofrece el mejor **compromiso general** entre ambas dimensiones, y es el candidato preferente para la implementación final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9ec9f-d304-4591-b032-0cecb41a03c8",
   "metadata": {},
   "source": [
    "<a id=\"section-seven-subsection-two\"></a>\n",
    "### **7.2 Selección del modelo con mejor balance interpretabilidad-rendimiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3b125-96f9-4c4d-8a7d-6c5009f67821",
   "metadata": {},
   "source": [
    "## Justificación de la selección del modelo\n",
    "\n",
    "Luego de analizar comparativamente el rendimiento de todos los modelos evaluados, se selecciona **XGBoost con hiperparámetros optimizados mediante GridSearchCV** como el modelo final para la predicción de aceptación de campañas de marketing.\n",
    "\n",
    "### Criterios de selección\n",
    "\n",
    "La elección se fundamenta en un conjunto de métricas y aspectos clave:\n",
    "\n",
    "- **F1 Score = 0.6454**: el más alto entre todos los modelos probados, lo que refleja el mejor equilibrio entre precisión y recall en el contexto de clases desbalanceadas.\n",
    "- **Recall = 0.8071**: el modelo logra capturar más del 80% de los clientes que efectivamente aceptan la oferta, lo cual es fundamental si se busca maximizar la tasa de respuesta.\n",
    "- **AUC = 0.9485**: confirma una capacidad de discriminación sobresaliente entre las clases positivas y negativas.\n",
    "- **Precision = 0.5377**: aunque moderada, se considera aceptable en el contexto del marketing directo, donde el costo de contactar falsos positivos es relativamente bajo frente al beneficio de captar nuevos clientes.\n",
    "- **Accuracy = 0.90**: alto nivel de aciertos globales, aunque este valor se interpreta con cautela debido al desbalance de clases.\n",
    "- **Robustez y estabilidad**: el modelo mostró un rendimiento estable tanto en validación cruzada como en test, sin signos evidentes de sobreajuste.\n",
    "- **Interpretabilidad**: mediante herramientas como SHAP y LIME se logró una buena comprensión del funcionamiento interno del modelo, fortaleciendo su confiabilidad.\n",
    "\n",
    "### Comparación con otros modelos\n",
    "\n",
    "Aunque modelos como **Random Forest** y **Regresión Logística** ofrecieron buenos resultados, no lograron superar al modelo XGBoost en términos de F1 Score y recall, métricas críticas para este caso de uso.\n",
    "\n",
    "En contraste, modelos como **Naive Bayes** o **KNN** mostraron un desempeño significativamente inferior y fueron descartados.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "El modelo XGBoost tuneado se posiciona como la solución más **eficiente, equilibrada y robusta** para predecir la aceptación de campañas de marketing en el presente caso, y será utilizado como base para la implementación final.\n",
    "\n",
    "\n",
    "> Por lo tanto, se selecciona el modelo **XGBoost con balanceo, escalado y selección automática de variables** como el modelo final del presente trabajo, al maximizar la capacidad del sistema para identificar clientes que efectivamente aceptarán una campaña, favoreciendo así la eficiencia en la gestión comercial del banco.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447748d8-a5e1-4d90-856b-09af6b30067c",
   "metadata": {},
   "source": [
    "<a id=\"section-eight\"></a>\n",
    "<h2><strong>8- PRODUCTIVIZACIÓN DEL MODELO</strong></h2>\n",
    "\n",
    "<p>\n",
    "La productivización del modelo es el proceso de preparar y desplegar el modelo entrenado para su uso en entornos reales, garantizando que funcione de manera eficiente, escalable y mantenible.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "En este TFM, se busca que el modelo predictivo pueda integrarse en un sistema que reciba datos nuevos, aplique el preprocesamiento adecuado y entregue predicciones automáticas. Para ello, se guarda el pipeline completo (preprocesamiento y modelo) usando herramientas como <code>joblib</code>, facilitando su carga y uso en producción.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Este paso es clave para transformar el trabajo de investigación en una solución práctica que aporte valor al banco, permitiendo la toma de decisiones basada en datos en tiempo real.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3e43b-2d76-484e-ad31-2b9ff0a67757",
   "metadata": {},
   "source": [
    "Primero se comienza por replicar el **preprocesamiento** utilizado en las etapasde desarrollo de este Notebook que permitirá transformar los datos ingresados en el chatbot del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197eb675-9c5f-457c-a485-05727d3bf09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Clase de preprocesamiento manual personalizado\n",
    "# ===============================\n",
    "\n",
    "class PreprocessingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Reglas personalizadas\n",
    "        X.loc[(X['age'] > 60) & (X['job'] == 'unknown'), 'job'] = 'retired'\n",
    "        X.loc[(X['education'] == 'unknown') & (X['job'] == 'management'), 'education'] = 'university.degree'\n",
    "        X.loc[(X['education'] == 'unknown') & (X['job'] == 'services'), 'education'] = 'high.school'\n",
    "        X.loc[(X['education'] == 'unknown') & (X['job'] == 'housemaid'), 'education'] = 'basic.4y'\n",
    "        X.loc[(X['job'] == 'unknown') & (X['education'] == 'basic.4y'), 'job'] = 'blue-collar'\n",
    "        X.loc[(X['job'] == 'unknown') & (X['education'] == 'basic.6y'), 'job'] = 'blue-collar'\n",
    "        X.loc[(X['job'] == 'unknown') & (X['education'] == 'basic.9y'), 'job'] = 'blue-collar'\n",
    "        X.loc[(X['job'] == 'unknown') & (X['education'] == 'professional.course'), 'job'] = 'technician'\n",
    "\n",
    "        # Transformaciones simples\n",
    "        X['pdays'] = X['pdays'].apply(lambda x: 0 if x == 999 else x)\n",
    "\n",
    "        X['month'].replace(\n",
    "            ('jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'),\n",
    "            range(1, 13), inplace=True)\n",
    "\n",
    "        X['day_of_week'].replace(\n",
    "            ('mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun'),\n",
    "            range(1, 8), inplace=True)\n",
    "\n",
    "        # Binning de edad\n",
    "        bins = [0, 25, 40, 60, 140]\n",
    "        labels = ['young', 'lower middle aged', 'middle aged', 'senior']\n",
    "        X['age_binned'] = pd.cut(X['age'], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "        # Agrupamientos\n",
    "        job_map = {\n",
    "            'admin.': 'White-collar', 'management': 'White-collar', 'technician': 'White-collar',\n",
    "            'blue-collar': 'Blue-collar', 'services': 'Blue-collar', 'housemaid': 'Blue-collar',\n",
    "            'entrepreneur': 'Self-employed', 'self-employed': 'Self-employed',\n",
    "            'retired': 'Non-active', 'student': 'Non-active', 'unemployed': 'Non-active',\n",
    "            'unknown': 'Other'\n",
    "        }\n",
    "        X['job_grouped'] = X['job'].map(job_map)\n",
    "\n",
    "        education_map = {\n",
    "            'basic.9y': 'Basic', 'basic.4y': 'Basic', 'basic.6y': 'Basic',\n",
    "            'high.school': 'Middle', 'professional.course': 'Middle',\n",
    "            'university.degree': 'Superior', 'unknown': 'Other', 'illiterate': 'Other'\n",
    "        }\n",
    "        X['education_grouped'] = X['education'].map(education_map)\n",
    "\n",
    "        # Nuevas variables combinadas\n",
    "        X['contacted_previously'] = (X['previous'] >= 1).astype(int)\n",
    "        X['life_stage'] = X['age_binned'].astype(str) + ' & ' + X['marital']\n",
    "        X['socio-economic'] = X['job'].astype(str) + ' & ' + X['education']\n",
    "\n",
    "        # Drop de columnas redundantes\n",
    "        X.drop(columns=['nr.employed'], inplace=True)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdacbb4-8ab4-4b77-a9ff-abff9779a8d8",
   "metadata": {},
   "source": [
    "Luego se continúa por establecer el mecanismo por el cual se **seleccionan solo un conjunto determinado de features**, las cuales coinciden con las features con las que se entrenó el modelo final optimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902ea7c-12ed-42f4-be14-93f0772cbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2. Clase FeatureSelector personalizada\n",
    "# ===============================\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.feature_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74382ed-5985-4e62-b002-b1c5f130beaf",
   "metadata": {},
   "source": [
    "Posteriormente, se reproduce un proceso de carga de datos, a los cuales se les quita la variable objetivo y se obtienen sus variables según sus tipologías específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba62551-7021-4a41-b7e4-d248aca5cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3. Preparación de los datos\n",
    "# ===============================\n",
    "\n",
    "#Efectúo la lectura del csv y guardo todo en un dataframe de Pandas con nombre df\n",
    "X = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "\n",
    "y = X['y'].replace({'no': 0, 'yes': 1})\n",
    "X = X.drop(columns=['y'])\n",
    "\n",
    "# Cargar las features seleccionadas previamente (después del encoding)\n",
    "selected_features_rfecv = joblib.load('selected_features_rfecv.pkl')\n",
    "\n",
    "model_final = joblib.load(\"modelo_final_xgb.pkl\")\n",
    "\n",
    "# Aplicar PreprocessingTransformer para generar nuevas variables\n",
    "X_temp = PreprocessingTransformer().fit_transform(X)\n",
    "\n",
    "# Detectar tipo de variables sobre el resultado del paso 1 (antes de codificar)\n",
    "numeric_cols = X_temp.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_temp.select_dtypes(include=['object', 'category']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f839a4-3299-4f7b-9db8-f9f39df168b4",
   "metadata": {},
   "source": [
    "Para manejar las variables categóricas en el pipeline de preprocesamiento, vuelvo a utilizar la técnica de **codificación One Hot Encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb28df0-a4a6-4160-ae78-9937a759eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4. OneHotEncoder personalizado como Transformer\n",
    "# ===============================\n",
    "\n",
    "#Clase que implementa un transformer para codificar variables categóricas mediante One Hot Encoding, integrable en pipelines de scikit-learn.\n",
    "class OneHotEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_cols):\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_ = X[self.categorical_cols].astype(str)\n",
    "        self.encoder.fit(X_)\n",
    "        self.feature_names_out = self.encoder.get_feature_names_out(self.categorical_cols)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X_cat = X[self.categorical_cols].astype(str)\n",
    "        encoded = self.encoder.transform(X_cat)\n",
    "        df_encoded = pd.DataFrame(encoded, columns=self.feature_names_out, index=X.index)\n",
    "        X.drop(columns=self.categorical_cols, inplace=True)\n",
    "        X = pd.concat([X, df_encoded], axis=1)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9923854-5d61-4836-969a-88613b485d22",
   "metadata": {},
   "source": [
    "Ahora, con la siguiente clase `ManualScaler` implemento un transformer compatible con scikit-learn que aplica **`MinMaxScaler`** únicamente a las columnas numéricas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892cf44-fb55-4ecd-b2e7-8dfb762fc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5. Transformador personalizado para escalado manual de variables numéricas \n",
    "# ===============================\n",
    "\n",
    "# Transformer personalizado que aplica MinMaxScaler únicamente a las columnas numéricas indicadas.\n",
    "class ManualScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.numeric_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.numeric_cols] = self.scaler.transform(X[self.numeric_cols])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75061a-5231-4764-8b96-7c73766903f7",
   "metadata": {},
   "source": [
    "En esta siguiente etapa se **construye el pipeline completo** que integra todas las fases del preprocesamiento personalizado, la codificación de variables categóricas, la selección de características, el escalado de variables numéricas y el modelo final resultante.\n",
    "\n",
    "El pipeline se entrena con los datos disponibles y, una vez ajustado, se guarda en disco utilizando `joblib`. Esto permite reutilizar el pipeline entrenado para realizar predicciones sobre datos nuevos sin necesidad de repetir todo el proceso desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d9341-eb34-4ecf-9023-71e8e9a45fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6. Construcción, entrenamiento y guardado del pipeline final\n",
    "# ===============================\n",
    "\n",
    "# Reutilizo las clases que generé antes\n",
    "pipeline_final = Pipeline([\n",
    "    ('manual_preprocessing', PreprocessingTransformer()),\n",
    "    ('encoding', OneHotEncoderTransformer(categorical_cols=categorical_cols)),\n",
    "    ('feature_selection', FeatureSelector(selected_features_rfecv)),\n",
    "    ('scaling', ManualScaler(numeric_cols=numeric_cols)),\n",
    "    ('modelo', model_final)\n",
    "])\n",
    "\n",
    "# Ajusto el pipeline\n",
    "pipeline_final.fit(X, y)\n",
    "\n",
    "# Guardado para producción\n",
    "joblib.dump(pipeline_final, 'pipeline_modelo_completo.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b831d6b-abd0-4ae6-a4f1-237897f7a651",
   "metadata": {},
   "source": [
    "Una vez guardado el pipeline completo (que incluye todas las transformaciones y el modelo entrenado), es posible cargarlo para su uso posterior sin necesidad de reentrenar.\n",
    "\n",
    "La celda de código a continuación, **inspecciona sus componentes internos para verificar las etapas y objetos que contiene el pipeline completo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99b325-c155-423a-92d6-37ab891277f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 7. Carga del pipeline guardado y acceso/revisión de sus componentes\n",
    "# ===============================\n",
    "\n",
    "# Cargo el pipeline completo previamente guardado, que incluye preprocesamiento y modelo\n",
    "modelo = joblib.load(\"pipeline_modelo_completo.pkl\")\n",
    "# Visualización de los pasos del pipeline para verificar su estructura y componentes\n",
    "modelo.named_steps  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf9327-d35d-4bd8-9bf0-abb35376ff37",
   "metadata": {},
   "source": [
    "### Desglose de los componentes del pipeline cargado\n",
    "\n",
    "Al cargar el pipeline completo guardado, podemos inspeccionar sus etapas internas con `named_steps`. \n",
    "\n",
    "El output muestra que el pipeline contiene las siguientes fases:\n",
    "\n",
    "- **manual_preprocessing**: Transformer personalizado para aplicar reglas manuales de preprocesamiento.\n",
    "- **encoding**: Transformer que realiza One Hot Encoding sobre las columnas categóricas indicadas.\n",
    "- **feature_selection**: Selector de características basado en la lista de features seleccionadas tras análisis previo.\n",
    "- **scaling**: Escalador personalizado que aplica MinMaxScaler solo a las variables numéricas especificadas.\n",
    "- **modelo**: El modelo entrenado final, en este caso un `XGBClassifier` con parámetros ajustados.\n",
    "\n",
    "Este desglose permite verificar que todas las etapas necesarias para el procesamiento y predicción están correctamente integradas en el pipeline, facilitando su uso y mantenimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a41329-846f-4eea-84e1-b74d558e99fc",
   "metadata": {},
   "source": [
    "### Detalle de implementación productiva final\n",
    "\n",
    "Finalmente, se señala que en el repositorio de GitHub previamente referenciado se incluye el archivo **app.py**, el cual constituye el núcleo del backend de la aplicación productiva desarrollada. Dicho backend, en conjunto con otros módulos y un frontend interactivo, posibilita la integración del modelo predictivo con un chatbot capaz de responder consultas del usuario mediante un LLM. Este LLM se alimenta de información obtenida a través del método RAG, a partir de chunks extraídos del informe final del TFM en formato PDF."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
